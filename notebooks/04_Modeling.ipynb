{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39179e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec1d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features =  ['SUM(transactions.Revenue)', 'SUM(transactions.Quantity)', 'COUNT(transactions) + MAX(transactions.Quantity)', 'total_orders', 'COUNT(transactions) + MAX(transactions.Revenue)', 'COUNT(transactions)', 'purchase_frequency', 'active_days', 'avg_days_between_orders', 'avg_items_per_order', 'avg_order_value', 'MAX(transactions.Revenue)', 'MEAN(transactions.Revenue)', 'MAX(transactions.Quantity) + MEAN(transactions.Revenue)']\n",
    "train = pd.read_csv('../data/features/train_final.csv')\n",
    "val = pd.read_csv('../data/features/val_final.csv')\n",
    "test = pd.read_csv('../data/features/test_final.csv')\n",
    "\n",
    "y_train = train['CLV_Target']\n",
    "X_train = train[selected_features]\n",
    "\n",
    "y_val = val['CLV_Target']\n",
    "X_val = val[selected_features]\n",
    "\n",
    "y_test = test['CLV_Target']\n",
    "X_test = test[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc75d8",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "- To start the modeling process I will train a ridge regression using cross validation - this will act as the baseline model to compare our ensembled methods against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d830fbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Training RMSE: 1516.4330\n",
      "Ridge Validation RMSE: 1269.2618\n"
     ]
    }
   ],
   "source": [
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, selected_features)\n",
    "    ]\n",
    ")\n",
    "pipeline_ridge = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5))\n",
    "])\n",
    "\n",
    "pipeline_ridge.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "y_val_pred_ridge = pipeline_ridge.predict(X_val)\n",
    "\n",
    "ridge_train_rsme = np.sqrt(mean_squared_error(y_train, pipeline_ridge.predict(X_train)))\n",
    "print(f'Ridge Training RMSE: {ridge_train_rsme:.4f}')\n",
    "ridge_val_rsme = np.sqrt(mean_squared_error(y_val, y_val_pred_ridge))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "val_r2 = pipeline_ridge.score(X_val, y_val)\n",
    "\n",
    "print(f'Ridge Validation RMSE: {ridge_val_rsme:.4f}')\n",
    "\n",
    "best_ridge = pipeline_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b211a",
   "metadata": {},
   "source": [
    "### Next we will repeat the above but using MLflow to track and log models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "738c9000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753f9807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/12/16 12:58:39 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Training RMSE: 1516.4330\n",
      "Ridge Validation RMSE: 1269.2618\n",
      "Ridge Validation R²: 0.7664\n",
      "Best Alpha: 10.0\n",
      "\n",
      "MLflow Run ID: ef49639c0c3b4a19bb573e012da8e2bf\n"
     ]
    }
   ],
   "source": [
    "# Set experiment name (organizes related runs)\n",
    "mlflow.set_experiment(\"CLV_Baseline_Models\")\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"Ridge_Baseline\"):\n",
    "    \n",
    "    # Log model parameters\n",
    "    mlflow.log_param(\"model_type\", \"ridge\")\n",
    "    mlflow.log_param(\"cv_folds\", 5)\n",
    "    mlflow.log_param(\"alphas\", \"[0.1, 1.0, 10.0]\")\n",
    "    mlflow.log_param(\"imputer_strategy\", \"median\")\n",
    "    mlflow.log_param(\"scaler\", \"StandardScaler\")\n",
    "    mlflow.log_param(\"n_features\", len(selected_features))\n",
    "    \n",
    "    # Log dataset info\n",
    "    mlflow.log_params({\n",
    "        \"train_samples\": len(X_train),\n",
    "        \"val_samples\": len(X_val),\n",
    "    })\n",
    "    \n",
    "    # Retraining the model\n",
    "    num_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_transformer, selected_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline_ridge = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RidgeCV(alphas=[0.1, 1.0, 10.0, 15], cv=5))\n",
    "    ])\n",
    "    \n",
    "    # Fit model\n",
    "    pipeline_ridge.fit(X_train, y_train)\n",
    "    \n",
    "    # Log best alpha\n",
    "    best_alpha = pipeline_ridge.named_steps['model'].alpha_\n",
    "    mlflow.log_param(\"best_alpha\", best_alpha)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_train_pred = pipeline_ridge.predict(X_train)\n",
    "    y_val_pred = pipeline_ridge.predict(X_val)\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    # Training metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Validation metrics\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Overfitting metrics\n",
    "    rmse_gap = val_rmse - train_rmse\n",
    "    r2_gap = train_r2 - val_r2\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"train_r2\": train_r2,\n",
    "        \"val_rmse\": val_rmse,\n",
    "        \"val_mae\": val_mae,\n",
    "        \"val_r2\": val_r2,\n",
    "        \"rmse_gap\": rmse_gap,\n",
    "        \"r2_gap\": r2_gap\n",
    "    })\n",
    "    \n",
    "    # Log features used\n",
    "    feature_dict = {\n",
    "        \"selected_features\": selected_features,\n",
    "        \"n_features\": len(selected_features),\n",
    "        \"feature_selection_method\": \"voting_ensemble\"\n",
    "    }\n",
    "    mlflow.log_dict(feature_dict, \"selected_features.json\")\n",
    "    \n",
    "    # Log the model\n",
    "    # Create signature (defines input/output schema)\n",
    "    signature = infer_signature(X_train[selected_features], y_train_pred)\n",
    "    \n",
    "    mlflow.sklearn.log_model(\n",
    "        pipeline_ridge,\n",
    "        \"ridge_model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train[selected_features].iloc[:5]  # Log sample input\n",
    "    )\n",
    "    \n",
    "    # Add tags\n",
    "    mlflow.set_tags({\n",
    "        \"stage\": \"baseline\",\n",
    "        \"model_family\": \"linear\",\n",
    "        \"feature_selection\": \"completed\",\n",
    "        \"author\": \"Bread\"\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Ridge Training RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"Ridge Validation RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"Ridge Validation R²: {val_r2:.4f}\")\n",
    "    print(f\"Best Alpha: {best_alpha}\")\n",
    "    print(f\"\\nMLflow Run ID: {mlflow.active_run().info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e80908",
   "metadata": {},
   "source": [
    "# Polynomial Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10472684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/12/16 17:06:21 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING RIDGE WITH POLYNOMIAL FEATURES\n",
      "======================================================================\n",
      "\n",
      "Training model...\n",
      "Created 119 polynomial features from 14 original features\n",
      "Best alpha: 15.0\n",
      "\n",
      "======================================================================\n",
      "RIDGE (POLYNOMIAL FEATURES) SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Feature Engineering:\n",
      "  Original features: 14\n",
      "  Polynomial features: 119\n",
      "  Polynomial degree: 2\n",
      "\n",
      "Best Hyperparameters:\n",
      "  Alpha: 15.0\n",
      "\n",
      "Performance:\n",
      "  Training RMSE: 1140.7323\n",
      "  Validation RMSE: 1350.6125\n",
      "  Validation R²: 0.7355\n",
      "  RMSE Gap: 209.8802\n",
      "  Training Time: 0.09s\n",
      "======================================================================\n",
      "\n",
      "✅ Ridge (Polynomial Features) logged to MLflow!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from mlflow.models import infer_signature\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ========== SET EXPERIMENT ==========\n",
    "mlflow.set_experiment(\"CLV_Baseline_Models\")\n",
    "\n",
    "# ========== START RUN ==========\n",
    "with mlflow.start_run(run_name=\"Ridge_PolynomialFeatures\"):\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"TRAINING RIDGE WITH POLYNOMIAL FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ========== LOG PARAMETERS ==========\n",
    "    poly_degree = 2\n",
    "    alphas = [0.1, 1.0, 10.0, 15.0]\n",
    "    \n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"ridge_polynomial\",\n",
    "        \"poly_degree\": poly_degree,\n",
    "        \"include_bias\": False,\n",
    "        \"alphas\": str(alphas),\n",
    "        \"cv_folds\": 5,\n",
    "        \"imputer_strategy\": \"median\",\n",
    "        \"scaler\": \"StandardScaler\",\n",
    "        \"n_original_features\": len(selected_features),\n",
    "        \"train_samples\": len(X_train),\n",
    "        \"val_samples\": len(X_val)\n",
    "    })\n",
    "    \n",
    "    # ========== BUILD PIPELINE ==========\n",
    "    num_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly_features', PolynomialFeatures(degree=poly_degree, include_bias=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_transformer, selected_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline_ridge = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RidgeCV(alphas=alphas, cv=5))\n",
    "    ])\n",
    "    \n",
    "    # ========== TRAIN MODEL ==========\n",
    "    print(\"\\nTraining model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pipeline_ridge.fit(X_train, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "    \n",
    "    best_poly_ridge = pipeline_ridge\n",
    "    # ========== LOG BEST ALPHA ==========\n",
    "    best_alpha = pipeline_ridge.named_steps['model'].alpha_\n",
    "    mlflow.log_param(\"best_alpha\", best_alpha)\n",
    "    \n",
    "    # Calculate number of polynomial features created\n",
    "    poly_transformer = pipeline_ridge.named_steps['preprocessor'].named_transformers_['num'].named_steps['poly_features']\n",
    "    n_poly_features = poly_transformer.n_output_features_\n",
    "    mlflow.log_param(\"n_poly_features\", n_poly_features)\n",
    "    \n",
    "    print(f\"Created {n_poly_features} polynomial features from {len(selected_features)} original features\")\n",
    "    print(f\"Best alpha: {best_alpha}\")\n",
    "    \n",
    "    # ========== PREDICTIONS ==========\n",
    "    y_train_pred = pipeline_ridge.predict(X_train)\n",
    "    y_val_pred = pipeline_ridge.predict(X_val)\n",
    "    \n",
    "    # ========== CALCULATE METRICS ==========\n",
    "    # Training metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Validation metrics\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Overfitting metrics\n",
    "    rmse_gap = val_rmse - train_rmse\n",
    "    r2_gap = train_r2 - val_r2\n",
    "    rmse_ratio = val_rmse / train_rmse\n",
    "    \n",
    "    # ========== LOG ALL METRICS ==========\n",
    "    mlflow.log_metrics({\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"train_r2\": train_r2,\n",
    "        \"val_rmse\": val_rmse,\n",
    "        \"val_mae\": val_mae,\n",
    "        \"val_r2\": val_r2,\n",
    "        \"rmse_gap\": rmse_gap,\n",
    "        \"r2_gap\": r2_gap,\n",
    "        \"rmse_ratio\": rmse_ratio\n",
    "    })\n",
    "    \n",
    "    # ========== LOG MODEL ==========\n",
    "    signature = infer_signature(X_train[selected_features], y_train_pred)\n",
    "    \n",
    "    mlflow.sklearn.log_model(\n",
    "        pipeline_ridge,\n",
    "        \"ridge_poly_model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train[selected_features].iloc[:5]\n",
    "    )\n",
    "    \n",
    "    # ========== LOG FEATURE NAMES ==========\n",
    "    # Get polynomial feature names\n",
    "    poly_feature_names = poly_transformer.get_feature_names_out(selected_features)\n",
    "    mlflow.log_dict({\n",
    "        \"original_features\": selected_features,\n",
    "        \"poly_feature_names\": poly_feature_names.tolist()[:100]  # Log first 100 to avoid huge file\n",
    "    }, \"feature_names.json\")\n",
    "    \n",
    "    # ========== LOG COEFFICIENTS ==========\n",
    "    coefficients = pipeline_ridge.named_steps['model'].coef_\n",
    "    \n",
    "    # Get top 20 coefficients by absolute value\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': poly_feature_names,\n",
    "        'coefficient': coefficients\n",
    "    })\n",
    "    coef_df['abs_coefficient'] = coef_df['coefficient'].abs()\n",
    "    top_coefs = coef_df.nlargest(20, 'abs_coefficient')\n",
    "    \n",
    "    top_coefs.to_csv('top_coefficients.csv', index=False)\n",
    "    mlflow.log_artifact('top_coefficients.csv')\n",
    "    \n",
    "    # ========== VISUALIZATIONS ==========\n",
    "    \n",
    "    # 1. Top Coefficients Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = ['red' if x < 0 else 'green' for x in top_coefs['coefficient']]\n",
    "    plt.barh(range(len(top_coefs)), top_coefs['coefficient'], color=colors)\n",
    "    plt.yticks(range(len(top_coefs)), top_coefs['feature'], fontsize=8)\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title('Top 20 Ridge Coefficients (Polynomial Features)')\n",
    "    plt.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ridge_poly_coefficients.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('ridge_poly_coefficients.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Residual Plot\n",
    "    residuals = y_val - y_val_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_val_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot - Ridge (Polynomial Features)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ridge_poly_residuals.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('ridge_poly_residuals.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Predicted vs Actual\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_val, y_val_pred, alpha=0.5)\n",
    "    plt.plot([y_val.min(), y_val.max()], \n",
    "             [y_val.min(), y_val.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "    plt.xlabel('Actual CLV')\n",
    "    plt.ylabel('Predicted CLV')\n",
    "    plt.title('Predicted vs Actual - Ridge (Polynomial Features)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ridge_poly_pred_vs_actual.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('ridge_poly_pred_vs_actual.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # ========== SAVE PREDICTIONS ==========\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Actual_CLV': y_val.values,\n",
    "        'Predicted_CLV': y_val_pred,\n",
    "        'Residual': residuals,\n",
    "        'Absolute_Error': np.abs(residuals),\n",
    "        'Percentage_Error': (np.abs(residuals) / y_val.values) * 100\n",
    "    })\n",
    "    predictions_df.to_csv('validation_predictions.csv', index=False)\n",
    "    mlflow.log_artifact('validation_predictions.csv')\n",
    "    \n",
    "    # ========== TAGS ==========\n",
    "    mlflow.set_tags({\n",
    "        \"stage\": \"baseline\",\n",
    "        \"model_family\": \"linear\",\n",
    "        \"feature_engineering\": \"polynomial\",\n",
    "        \"author\": \"Bread\"\n",
    "    })\n",
    "    \n",
    "    # ========== PRINT SUMMARY ==========\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RIDGE (POLYNOMIAL FEATURES) SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nFeature Engineering:\")\n",
    "    print(f\"  Original features: {len(selected_features)}\")\n",
    "    print(f\"  Polynomial features: {n_poly_features}\")\n",
    "    print(f\"  Polynomial degree: {poly_degree}\")\n",
    "    \n",
    "    print(f\"\\nBest Hyperparameters:\")\n",
    "    print(f\"  Alpha: {best_alpha}\")\n",
    "    \n",
    "    print(f\"\\nPerformance:\")\n",
    "    print(f\"  Training RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"  Validation RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"  Validation R²: {val_r2:.4f}\")\n",
    "    print(f\"  RMSE Gap: {rmse_gap:.4f}\")\n",
    "    print(f\"  Training Time: {training_time:.2f}s\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"\\n✅ Ridge (Polynomial Features) logged to MLflow!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd1086c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model is already fitted\n",
    "best_poly_ridge = pipeline_ridge\n",
    "\n",
    "# Predict on any dataset with same columns\n",
    "y_val_poly = best_poly_ridge.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b4bd29",
   "metadata": {},
   "source": [
    "# Xgboost\n",
    "- As we are testing an ensembled method if may also be useful to train the individual models to compare them against our final one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de4f00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1a7b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV RMSE: 2159.9203623620842\n"
     ]
    }
   ],
   "source": [
    "num_transformer_xg = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "])\n",
    "\n",
    "preprocessor_xg = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer_xg, selected_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_xg = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_xg),\n",
    "    ('model', XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__n_estimators': [300, 500, 800],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__subsample': [0.6, 0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=pipeline_xg,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV RMSE:\", -search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030220e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Validation RMSE: 1247.8709\n",
      "XGBoost Validation R²: 0.7742\n"
     ]
    }
   ],
   "source": [
    "y_val_pred_xg = search.predict(X_val)\n",
    "val_rmse_xg = np.sqrt(mean_squared_error(y_val, y_val_pred_xg))\n",
    "val_r2_xg = r2_score(y_val, y_val_pred_xg)  \n",
    "print(f'XGBoost Validation RMSE: {val_rmse_xg:.4f}')\n",
    "print(f'XGBoost Validation R²: {val_r2_xg:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9defec17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Run ID: 4e0ef8e08fc548409934b9027f3443ae\n",
      "Starting RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=800, model__subsample=0.6; total time=   0.3s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=800, model__subsample=0.6; total time=   0.3s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=800, model__subsample=0.6; total time=   0.3s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=800, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=800, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=7, model__n_estimators=800, model__subsample=0.6; total time=   0.8s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=7, model__n_estimators=800, model__subsample=0.6; total time=   0.8s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=7, model__n_estimators=800, model__subsample=0.6; total time=   0.8s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=7, model__n_estimators=800, model__subsample=0.6; total time=   0.9s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=7, model__n_estimators=800, model__subsample=0.6; total time=   1.0s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=1.0; total time=   0.9s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=1.0; total time=   0.8s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=1.0; total time=   0.9s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=1.0; total time=   0.9s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=1.0; total time=   0.9s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=0.8; total time=   1.0s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=0.8; total time=   1.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=0.8; total time=   1.3s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=0.8; total time=   1.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=500, model__subsample=0.8; total time=   0.3s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=0.8; total time=   1.2s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=500, model__subsample=0.8; total time=   0.3s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=500, model__subsample=0.8; total time=   0.3s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=500, model__subsample=0.8; total time=   0.3s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.2, model__max_depth=7, model__n_estimators=500, model__subsample=0.8; total time=   1.6s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=500, model__subsample=0.8; total time=   0.3s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.2, model__max_depth=7, model__n_estimators=500, model__subsample=0.8; total time=   1.5s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.2, model__max_depth=7, model__n_estimators=500, model__subsample=0.8; total time=   1.5s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.2, model__max_depth=7, model__n_estimators=500, model__subsample=0.8; total time=   1.6s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.2, model__max_depth=7, model__n_estimators=500, model__subsample=0.8; total time=   1.4s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=0.6; total time=   0.7s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=0.6; total time=   0.7s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=0.6; total time=   0.8s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=300, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=300, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=0.6; total time=   0.9s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.1, model__max_depth=7, model__n_estimators=800, model__subsample=1.0; total time=   2.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=300, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.2, model__max_depth=5, model__n_estimators=800, model__subsample=0.6; total time=   0.9s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.1, model__max_depth=7, model__n_estimators=800, model__subsample=1.0; total time=   2.2s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.1, model__max_depth=7, model__n_estimators=800, model__subsample=1.0; total time=   2.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=300, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=500, model__subsample=1.0; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.6, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=300, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=500, model__subsample=1.0; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=500, model__subsample=1.0; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=500, model__subsample=1.0; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=500, model__subsample=1.0; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.1, model__max_depth=7, model__n_estimators=800, model__subsample=1.0; total time=   2.2s\n",
      "[CV] END model__colsample_bytree=1.0, model__learning_rate=0.1, model__max_depth=7, model__n_estimators=800, model__subsample=1.0; total time=   2.1s\n",
      "\n",
      "Search completed in 8.64 seconds\n",
      "Best CV RMSE: 2159.9204\n",
      "\n",
      "Logging individual CV trials as nested runs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/12/16 17:05:06 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged 10 nested runs\n",
      "\n",
      "Training final model with best parameters...\n",
      "\n",
      "======================================================================\n",
      "XGBOOST RANDOMIZED SEARCH SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Best Parameters:\n",
      "  subsample: 0.6\n",
      "  n_estimators: 800\n",
      "  max_depth: 3\n",
      "  learning_rate: 0.01\n",
      "  colsample_bytree: 0.6\n",
      "\n",
      "Cross-Validation Results:\n",
      "  Best CV RMSE: 2159.9204\n",
      "\n",
      "Final Model Performance:\n",
      "  Training RMSE: 837.1703\n",
      "  Validation RMSE: 1247.8709\n",
      "  Validation R²: 0.7742\n",
      "  RMSE Gap (Val - Train): 410.7007\n",
      "\n",
      "MLflow Tracking:\n",
      "  Parent Run ID: 4e0ef8e08fc548409934b9027f3443ae\n",
      "  Nested Runs: 10\n",
      "  View at: http://localhost:5000\n",
      "======================================================================\n",
      "\n",
      "✅ XGBoost RandomizedSearchCV logged to MLflow!\n",
      "Best CV RMSE: 2159.9204\n"
     ]
    }
   ],
   "source": [
    "# Log model with MLflow\n",
    "import time\n",
    "\n",
    "\n",
    "mlflow.set_experiment(\"CLV_Baseline_Models\")\n",
    "\n",
    "# ========== START PARENT RUN ==========\n",
    "with mlflow.start_run(run_name=\"XGBoost_RandomizedSearch\") as parent_run:\n",
    "    \n",
    "    print(f\"Parent Run ID: {parent_run.info.run_id}\")\n",
    "    \n",
    "    # ========== LOG SEARCH CONFIGURATION ==========\n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"xgboost\",\n",
    "        \"search_method\": \"randomized_search\",\n",
    "        \"n_iter\": 10,\n",
    "        \"cv_folds\": 5,\n",
    "        \"scoring\": \"neg_root_mean_squared_error\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_features\": len(selected_features),\n",
    "        \"train_samples\": len(X_train)\n",
    "    })\n",
    "    \n",
    "    # Log parameter grid\n",
    "    mlflow.log_dict({\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [300, 500, 800],\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "            \"subsample\": [0.6, 0.8, 1.0],\n",
    "            \"colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "        }\n",
    "    }, \"param_grid.json\")\n",
    "    \n",
    "    # ========== BUILD PIPELINE ==========\n",
    "    num_transformer_xg = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "    ])\n",
    "    \n",
    "    preprocessor_xg = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_transformer_xg, selected_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline_xg = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor_xg),\n",
    "        ('model', XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # ========== DEFINE PARAMETER GRID ==========\n",
    "    param_grid = {\n",
    "        'model__n_estimators': [300, 500, 800],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__subsample': [0.6, 0.8, 1.0],\n",
    "        'model__colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    # ========== RUN RANDOMIZED SEARCH ==========\n",
    "    print(\"Starting RandomizedSearchCV...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipeline_xg,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=10,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=2  # Show progress\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    best_XG_model = search.best_estimator_\n",
    "    \n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nSearch completed in {search_time:.2f} seconds\")\n",
    "    print(f\"Best CV RMSE: {-search.best_score_:.4f}\")\n",
    "    \n",
    "    # ========== LOG SEARCH RESULTS TO PARENT RUN ==========\n",
    "    mlflow.log_metric(\"search_time_seconds\", search_time)\n",
    "    mlflow.log_metric(\"best_cv_rmse\", -search.best_score_)\n",
    "    \n",
    "    # Log best parameters\n",
    "    best_params = {k.replace('model__', ''): v \n",
    "                   for k, v in search.best_params_.items()}\n",
    "    mlflow.log_params({f\"best_{k}\": v for k, v in best_params.items()})\n",
    "    \n",
    "    # ========== LOG ALL CV RESULTS AS NESTED RUNS ==========\n",
    "    print(\"\\nLogging individual CV trials as nested runs...\")\n",
    "    \n",
    "    cv_results = pd.DataFrame(search.cv_results_)\n",
    "    \n",
    "    for idx in range(len(cv_results)):\n",
    "        with mlflow.start_run(run_name=f\"trial_{idx}\", nested=True):\n",
    "            \n",
    "            # Extract parameters for this trial\n",
    "            trial_params = {}\n",
    "            for param_name in param_grid.keys():\n",
    "                param_key = f\"param_{param_name}\"\n",
    "                if param_key in cv_results.columns:\n",
    "                    value = cv_results.loc[idx, param_key]\n",
    "                    # Remove 'model__' prefix for cleaner logging\n",
    "                    clean_name = param_name.replace('model__', '')\n",
    "                    trial_params[clean_name] = value\n",
    "            \n",
    "            # Log trial parameters\n",
    "            mlflow.log_params(trial_params)\n",
    "            \n",
    "            # Log CV metrics\n",
    "            mean_test_score = -cv_results.loc[idx, 'mean_test_score']\n",
    "            std_test_score = cv_results.loc[idx, 'std_test_score']\n",
    "            \n",
    "            mlflow.log_metrics({\n",
    "                \"cv_rmse_mean\": mean_test_score,\n",
    "                \"cv_rmse_std\": std_test_score,\n",
    "                \"rank\": cv_results.loc[idx, 'rank_test_score']\n",
    "            })\n",
    "            \n",
    "            # Log individual fold scores\n",
    "            for fold_idx in range(5):  # 5 folds\n",
    "                fold_key = f'split{fold_idx}_test_score'\n",
    "                if fold_key in cv_results.columns:\n",
    "                    fold_score = -cv_results.loc[idx, fold_key]\n",
    "                    mlflow.log_metric(f\"cv_rmse_fold_{fold_idx}\", fold_score)\n",
    "            \n",
    "            # Tag\n",
    "            mlflow.set_tag(\"trial_number\", idx)\n",
    "            if idx == search.best_index_:\n",
    "                mlflow.set_tag(\"best_trial\", \"true\")\n",
    "    \n",
    "    print(f\"Logged {len(cv_results)} nested runs\")\n",
    "    \n",
    "    # ========== TRAIN FINAL MODEL WITH BEST PARAMS ==========\n",
    "    print(\"\\nTraining final model with best parameters...\")\n",
    "    \n",
    "    best_pipeline = search.best_estimator_\n",
    "    \n",
    "    # Evaluate on train and validation\n",
    "    y_train_pred = best_pipeline.predict(X_train)\n",
    "    y_val_pred = best_pipeline.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Log final model metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"final_train_rmse\": train_rmse,\n",
    "        \"final_train_mae\": train_mae,\n",
    "        \"final_train_r2\": train_r2,\n",
    "        \"final_val_rmse\": val_rmse,\n",
    "        \"final_val_mae\": val_mae,\n",
    "        \"final_val_r2\": val_r2,\n",
    "        \"rmse_gap\": val_rmse - train_rmse,\n",
    "        \"r2_gap\": train_r2 - val_r2\n",
    "    })\n",
    "    \n",
    "    # ========== LOG BEST MODEL ==========\n",
    "    signature = infer_signature(X_train[selected_features], y_train_pred)\n",
    "    \n",
    "    mlflow.sklearn.log_model(\n",
    "        best_pipeline,\n",
    "        \"best_xgboost_model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train[selected_features].iloc[:5]\n",
    "    )\n",
    "    \n",
    "    # ========== LOG FEATURE IMPORTANCE ==========\n",
    "    xgb_model = best_pipeline.named_steps['model']\n",
    "    feature_importance = xgb_model.feature_importances_\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Save as JSON\n",
    "    importance_dict = dict(zip(selected_features, feature_importance))\n",
    "    mlflow.log_dict(importance_dict, \"feature_importance.json\")\n",
    "    \n",
    "    # Save as CSV\n",
    "    importance_df.to_csv('feature_importance.csv', index=False)\n",
    "    mlflow.log_artifact('feature_importance.csv')\n",
    "    \n",
    "    # ========== VISUALIZATIONS ==========\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # 1. Feature Importance Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_20 = importance_df.head(20)\n",
    "    plt.barh(top_20['feature'], top_20['importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Top 20 Feature Importances (XGBoost)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgb_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('xgb_feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Residual Plot\n",
    "    residuals = y_val - y_val_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_val_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot - XGBoost')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgb_residuals.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('xgb_residuals.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Predicted vs Actual\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_val, y_val_pred, alpha=0.5)\n",
    "    plt.plot([y_val.min(), y_val.max()], \n",
    "             [y_val.min(), y_val.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "    plt.xlabel('Actual CLV')\n",
    "    plt.ylabel('Predicted CLV')\n",
    "    plt.title('Predicted vs Actual - XGBoost')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgb_pred_vs_actual.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('xgb_pred_vs_actual.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. CV Results Distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    cv_scores = -cv_results['mean_test_score']\n",
    "    plt.hist(cv_scores, bins=20, edgecolor='black', alpha=0.7)\n",
    "    plt.axvline(-search.best_score_, color='r', linestyle='--', \n",
    "                linewidth=2, label=f'Best: {-search.best_score_:.4f}')\n",
    "    plt.xlabel('CV RMSE')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Cross-Validation RMSE Scores')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cv_scores_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('cv_scores_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # ========== LOG CV RESULTS TABLE ==========\n",
    "    cv_results_summary = cv_results[[\n",
    "        'mean_test_score', 'std_test_score', 'rank_test_score',\n",
    "        'param_model__n_estimators', 'param_model__max_depth',\n",
    "        'param_model__learning_rate', 'param_model__subsample',\n",
    "        'param_model__colsample_bytree'\n",
    "    ]].copy()\n",
    "    \n",
    "    cv_results_summary['mean_test_score'] = -cv_results_summary['mean_test_score']\n",
    "    cv_results_summary = cv_results_summary.sort_values('rank_test_score')\n",
    "    cv_results_summary.to_csv('cv_results_summary.csv', index=False)\n",
    "    mlflow.log_artifact('cv_results_summary.csv')\n",
    "    \n",
    "    # ========== SAVE PREDICTIONS ==========\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Actual_CLV': y_val.values,\n",
    "        'Predicted_CLV': y_val_pred,\n",
    "        'Residual': residuals,\n",
    "        'Absolute_Error': np.abs(residuals),\n",
    "        'Percentage_Error': (np.abs(residuals) / y_val.values) * 100\n",
    "    })\n",
    "    predictions_df.to_csv('validation_predictions.csv', index=False)\n",
    "    mlflow.log_artifact('validation_predictions.csv')\n",
    "    \n",
    "    # ========== TAGS ==========\n",
    "    mlflow.set_tags({\n",
    "        \"stage\": \"hyperparameter_tuning\",\n",
    "        \"model_family\": \"gradient_boosting\",\n",
    "        \"search_type\": \"randomized\",\n",
    "        \"author\": \"Bread\",\n",
    "        \"best_model\": \"true\"\n",
    "    })\n",
    "    \n",
    "    # ========== PRINT SUMMARY ==========\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"XGBOOST RANDOMIZED SEARCH SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nBest Parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nCross-Validation Results:\")\n",
    "    print(f\"  Best CV RMSE: {-search.best_score_:.4f}\")\n",
    "    \n",
    "    print(f\"\\nFinal Model Performance:\")\n",
    "    print(f\"  Training RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"  Validation RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"  Validation R²: {val_r2:.4f}\")\n",
    "    print(f\"  RMSE Gap (Val - Train): {val_rmse - train_rmse:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMLflow Tracking:\")\n",
    "    print(f\"  Parent Run ID: {parent_run.info.run_id}\")\n",
    "    print(f\"  Nested Runs: {len(cv_results)}\")\n",
    "    print(f\"  View at: http://localhost:5000\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# ========== DONE ==========\n",
    "print(\"\\n✅ XGBoost RandomizedSearchCV logged to MLflow!\")\n",
    "print(f\"Best CV RMSE: {-search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0f2af",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64c2637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "411d2f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LIGHTGBM RANDOMIZED SEARCH\n",
      "======================================================================\n",
      "Parent Run ID: 15b9eb06e4214a29b958affa4ce77a75\n",
      "\n",
      "Starting RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search completed in 136.17 seconds\n",
      "Best CV RMSE: 2168.6199\n",
      "\n",
      "Logging individual CV trials as nested runs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/12/16 14:53:22 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged 20 nested runs\n",
      "\n",
      "Evaluating best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LIGHTGBM RANDOMIZED SEARCH SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Best Parameters:\n",
      "  subsample: 0.7\n",
      "  num_leaves: 63\n",
      "  n_estimators: 500\n",
      "  min_child_samples: 20\n",
      "  max_depth: 5\n",
      "  learning_rate: 0.05\n",
      "  colsample_bytree: 0.8\n",
      "\n",
      "Cross-Validation Results:\n",
      "  Best CV RMSE: 2168.6199\n",
      "\n",
      "Final Model Performance:\n",
      "  Training RMSE: 1526.0895\n",
      "  Validation RMSE: 1431.0001\n",
      "  Validation R²: 0.7031\n",
      "  RMSE Gap: -95.0894\n",
      "\n",
      "MLflow Tracking:\n",
      "  Parent Run ID: 15b9eb06e4214a29b958affa4ce77a75\n",
      "  Nested Runs: 20\n",
      "  View at: http://localhost:5000\n",
      "======================================================================\n",
      "\n",
      "✅ LightGBM RandomizedSearchCV logged to MLflow!\n"
     ]
    }
   ],
   "source": [
    "# ========== SET EXPERIMENT ==========\n",
    "mlflow.set_experiment(\"CLV_Baseline_Models\")\n",
    "\n",
    "# ========== START PARENT RUN ==========\n",
    "with mlflow.start_run(run_name=\"LightGBM_RandomizedSearch\") as parent_run:\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"LIGHTGBM RANDOMIZED SEARCH\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Parent Run ID: {parent_run.info.run_id}\")\n",
    "    \n",
    "    # ========== LOG SEARCH CONFIGURATION ==========\n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"lightgbm\",\n",
    "        \"search_method\": \"randomized_search\",\n",
    "        \"n_iter\": 20,\n",
    "        \"cv_folds\": 5,\n",
    "        \"scoring\": \"neg_root_mean_squared_error\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_features\": len(selected_features),\n",
    "        \"train_samples\": len(X_train),\n",
    "        \"val_samples\": len(X_val)\n",
    "    })\n",
    "    \n",
    "    # ========== DEFINE PARAMETER GRID ==========\n",
    "    param_grid = {\n",
    "        \"model__n_estimators\": [300, 500, 800],\n",
    "        \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"model__num_leaves\": [15, 31, 63],\n",
    "        \"model__max_depth\": [-1, 5, 10],\n",
    "        \"model__min_child_samples\": [10, 20, 50],\n",
    "        \"model__subsample\": [0.7, 0.8, 1.0],\n",
    "        \"model__colsample_bytree\": [0.7, 0.8, 1.0],\n",
    "    }\n",
    "    \n",
    "    # Log parameter grid\n",
    "    mlflow.log_dict({\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [300, 500, 800],\n",
    "            \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"num_leaves\": [15, 31, 63],\n",
    "            \"max_depth\": [-1, 5, 10],\n",
    "            \"min_child_samples\": [10, 20, 50],\n",
    "            \"subsample\": [0.7, 0.8, 1.0],\n",
    "            \"colsample_bytree\": [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    }, \"param_grid.json\")\n",
    "    \n",
    "    # ========== BUILD PIPELINE ==========\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, selected_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline_lgbm = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", LGBMRegressor(\n",
    "            objective=\"regression\",\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1  # Suppress LightGBM warnings\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # ========== RUN RANDOMIZED SEARCH ==========\n",
    "    print(\"\\nStarting RandomizedSearchCV...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    search_lgbm = RandomizedSearchCV(\n",
    "        estimator=pipeline_lgbm,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        cv=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    search_lgbm.fit(X_train, y_train)\n",
    "    \n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nSearch completed in {search_time:.2f} seconds\")\n",
    "    \n",
    "    # ========== LOG SEARCH RESULTS ==========\n",
    "    cv_rmse = -search_lgbm.best_score_\n",
    "    mlflow.log_metric(\"search_time_seconds\", search_time)\n",
    "    mlflow.log_metric(\"best_cv_rmse\", cv_rmse)\n",
    "    \n",
    "    # Extract and log best parameters\n",
    "    best_params = {k.replace('model__', ''): v \n",
    "                   for k, v in search_lgbm.best_params_.items()}\n",
    "    mlflow.log_params({f\"best_{k}\": v for k, v in best_params.items()})\n",
    "    \n",
    "    print(f\"Best CV RMSE: {cv_rmse:.4f}\")\n",
    "    \n",
    "    # ========== LOG NESTED RUNS FOR EACH TRIAL ==========\n",
    "    print(\"\\nLogging individual CV trials as nested runs...\")\n",
    "    \n",
    "    cv_results = pd.DataFrame(search_lgbm.cv_results_)\n",
    "    \n",
    "    for idx in range(len(cv_results)):\n",
    "        with mlflow.start_run(run_name=f\"trial_{idx}\", nested=True):\n",
    "            \n",
    "            # Extract parameters for this trial\n",
    "            trial_params = {}\n",
    "            for param_name in param_grid.keys():\n",
    "                param_key = f\"param_{param_name}\"\n",
    "                if param_key in cv_results.columns:\n",
    "                    value = cv_results.loc[idx, param_key]\n",
    "                    clean_name = param_name.replace('model__', '')\n",
    "                    trial_params[clean_name] = value\n",
    "            \n",
    "            # Log trial parameters\n",
    "            mlflow.log_params(trial_params)\n",
    "            \n",
    "            # Log CV metrics\n",
    "            mean_test_score = -cv_results.loc[idx, 'mean_test_score']\n",
    "            std_test_score = cv_results.loc[idx, 'std_test_score']\n",
    "            \n",
    "            mlflow.log_metrics({\n",
    "                \"cv_rmse_mean\": mean_test_score,\n",
    "                \"cv_rmse_std\": std_test_score,\n",
    "                \"rank\": cv_results.loc[idx, 'rank_test_score']\n",
    "            })\n",
    "            \n",
    "            # Log individual fold scores\n",
    "            for fold_idx in range(5):\n",
    "                fold_key = f'split{fold_idx}_test_score'\n",
    "                if fold_key in cv_results.columns:\n",
    "                    fold_score = -cv_results.loc[idx, fold_key]\n",
    "                    mlflow.log_metric(f\"cv_rmse_fold_{fold_idx}\", fold_score)\n",
    "            \n",
    "            # Tags\n",
    "            mlflow.set_tag(\"trial_number\", idx)\n",
    "            if idx == search_lgbm.best_index_:\n",
    "                mlflow.set_tag(\"best_trial\", \"true\")\n",
    "    \n",
    "    print(f\"Logged {len(cv_results)} nested runs\")\n",
    "    \n",
    "    # ========== EVALUATE BEST MODEL ==========\n",
    "    print(\"\\nEvaluating best model...\")\n",
    "    \n",
    "    best_lgbm = search_lgbm.best_estimator_\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = best_lgbm.predict(X_train)\n",
    "    y_val_pred = best_lgbm.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Log final metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"final_train_rmse\": train_rmse,\n",
    "        \"final_train_mae\": train_mae,\n",
    "        \"final_train_r2\": train_r2,\n",
    "        \"final_val_rmse\": val_rmse,\n",
    "        \"final_val_mae\": val_mae,\n",
    "        \"final_val_r2\": val_r2,\n",
    "        \"rmse_gap\": val_rmse - train_rmse,\n",
    "        \"r2_gap\": train_r2 - val_r2\n",
    "    })\n",
    "    \n",
    "    # ========== LOG BEST MODEL ==========\n",
    "    signature = infer_signature(X_train[selected_features], y_train_pred)\n",
    "    \n",
    "    mlflow.sklearn.log_model(\n",
    "        best_lgbm,\n",
    "        \"best_lgbm_model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train[selected_features].iloc[:5]\n",
    "    )\n",
    "    \n",
    "    # ========== LOG FEATURE IMPORTANCE ==========\n",
    "    lgbm_model = best_lgbm.named_steps['model']\n",
    "    feature_importance = lgbm_model.feature_importances_\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Save as JSON and CSV\n",
    "    importance_dict = dict(zip(selected_features, feature_importance))\n",
    "    mlflow.log_dict(importance_dict, \"feature_importance.json\")\n",
    "    \n",
    "    importance_df.to_csv('feature_importance.csv', index=False)\n",
    "    mlflow.log_artifact('feature_importance.csv')\n",
    "    \n",
    "    # ========== VISUALIZATIONS ==========\n",
    "    \n",
    "    # 1. Feature Importance Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_20 = importance_df.head(20)\n",
    "    plt.barh(top_20['feature'], top_20['importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Top 20 Feature Importances (LightGBM)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('lgbm_feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Residual Plot\n",
    "    residuals = y_val - y_val_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_val_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot - LightGBM')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_residuals.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('lgbm_residuals.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Predicted vs Actual\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_val, y_val_pred, alpha=0.5)\n",
    "    plt.plot([y_val.min(), y_val.max()], \n",
    "             [y_val.min(), y_val.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "    plt.xlabel('Actual CLV')\n",
    "    plt.ylabel('Predicted CLV')\n",
    "    plt.title('Predicted vs Actual - LightGBM')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_pred_vs_actual.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('lgbm_pred_vs_actual.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. CV Results Distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    cv_scores = -cv_results['mean_test_score']\n",
    "    plt.hist(cv_scores, bins=20, edgecolor='black', alpha=0.7)\n",
    "    plt.axvline(cv_rmse, color='r', linestyle='--', \n",
    "                linewidth=2, label=f'Best: {cv_rmse:.4f}')\n",
    "    plt.xlabel('CV RMSE')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Cross-Validation RMSE Scores')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_cv_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('lgbm_cv_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # ========== SAVE CV RESULTS ==========\n",
    "    cv_results_summary = cv_results[[\n",
    "        'mean_test_score', 'std_test_score', 'rank_test_score',\n",
    "        'param_model__n_estimators', 'param_model__learning_rate',\n",
    "        'param_model__num_leaves', 'param_model__max_depth',\n",
    "        'param_model__min_child_samples', 'param_model__subsample',\n",
    "        'param_model__colsample_bytree'\n",
    "    ]].copy()\n",
    "    \n",
    "    cv_results_summary['mean_test_score'] = -cv_results_summary['mean_test_score']\n",
    "    cv_results_summary = cv_results_summary.sort_values('rank_test_score')\n",
    "    cv_results_summary.to_csv('lgbm_cv_results.csv', index=False)\n",
    "    mlflow.log_artifact('lgbm_cv_results.csv')\n",
    "    \n",
    "    # ========== SAVE PREDICTIONS ==========\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Actual_CLV': y_val.values,\n",
    "        'Predicted_CLV': y_val_pred,\n",
    "        'Residual': residuals,\n",
    "        'Absolute_Error': np.abs(residuals),\n",
    "        'Percentage_Error': (np.abs(residuals) / y_val.values) * 100\n",
    "    })\n",
    "    predictions_df.to_csv('lgbm_predictions.csv', index=False)\n",
    "    mlflow.log_artifact('lgbm_predictions.csv')\n",
    "    \n",
    "    # ========== TAGS ==========\n",
    "    mlflow.set_tags({\n",
    "        \"stage\": \"baseline\",\n",
    "        \"model_family\": \"gradient_boosting\",\n",
    "        \"search_type\": \"randomized\",\n",
    "        \"author\": \"Bread\",\n",
    "        \"best_model\": \"true\"\n",
    "    })\n",
    "    \n",
    "    # ========== PRINT SUMMARY ==========\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LIGHTGBM RANDOMIZED SEARCH SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nBest Parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nCross-Validation Results:\")\n",
    "    print(f\"  Best CV RMSE: {cv_rmse:.4f}\")\n",
    "    \n",
    "    print(f\"\\nFinal Model Performance:\")\n",
    "    print(f\"  Training RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"  Validation RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"  Validation R²: {val_r2:.4f}\")\n",
    "    print(f\"  RMSE Gap: {val_rmse - train_rmse:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMLflow Tracking:\")\n",
    "    print(f\"  Parent Run ID: {parent_run.info.run_id}\")\n",
    "    print(f\"  Nested Runs: {len(cv_results)}\")\n",
    "    print(f\"  View at: http://localhost:5000\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"\\n✅ LightGBM RandomizedSearchCV logged to MLflow!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "feb761b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_lgbm = search_lgbm.best_estimator_\n",
    "\n",
    "y_val_pred_lgbm = best_lgbm.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4ee37b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESIDUAL CORRELATION MATRIX\n",
      "======================================================================\n",
      "          ridge      poly       xgb      lgbm\n",
      "ridge  1.000000  0.710140  0.749376  0.755849\n",
      "poly   0.710140  1.000000  0.697799  0.505384\n",
      "xgb    0.749376  0.697799  1.000000  0.649388\n",
      "lgbm   0.755849  0.505384  0.649388  1.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn0AAAJOCAYAAADCnbHOAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAdAlJREFUeJzt3QWcFPX7wPFn97qbRlIa6RILDFQQ668IKohgoGJggihgoT8LG1BAMBFBRVERUVQkBSnBo7uuu2//r+d77nJ7d8TBFczn/XrN3c53Zid3Zp/91tgcDodDAAAAcEazV/YGAAAAoPwR9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAFEPShVD788EOx2Wzm/4mqX7++Gar6dp5pxo4da47BokWLTut1ACdCP4P6WdTP5KmojPsVUFEI+k4DO3fuNDezwoOXl5fUrl1bbrzxRvnrr78qexPPGOnp6fLGG29Ijx49JCoqyhzn8PBwOe+88+TFF1+UmJgYsZKy+iKtKM4gtPDg4eEhkZGRctlll8k333xjuWNSnpzH2MfHR+Li4kqcJyEhQfz8/FzzAqg8npW4bpRSo0aN5JZbbjGv09LSZNWqVTJr1iz5+uuv5eeff5YLLrig3I/ptddeK127dpWaNWvKmWbt2rVy9dVXy65du6RevXrSt29fqV69uiQnJ8uyZctk5MiRMn78eNm/f78EBARU9uZWCffdd5/cdNNNctZZZ0lVcv3110urVq3M6+zsbNm2bZvMnTtXFixYIG+//bbce++9lb2JZwxPT09zjD/55BO5//77i03X9MzMTDNfbm5upWwjgAIEfaeRxo0bF8td0NwnDUaeeuop+e2338p9G0JCQsxwptm7d6/JCYqNjZVXX31VHnjgAZNDVNjff/9tgpycnJxK286qRnPQdKhq/u///s8Eo4WtXLlSOnfuLC+99BJBXxn/GHU4HDJt2rQSg76pU6dK06ZNzevo6OiyXDWAUqJ49zQ3ZMgQ819z/YrSX9+vvfaatG/f3uRMBQUFyfnnn29yPIpKSkqSp59+Wlq0aCGBgYESHBxsgsxBgwaZnK8TqSunRWedOnUyRTmaQ3bHHXeYop2SXHTRRUct6rntttvMNC3WLrx9+mV94YUXSq1atcTb29v8HzhwoMnFOVVPPvmkHD58WEaNGiUjRowoFvCpdu3amcBaj01h3377rSkO1mBY971NmzbmuBfN1XAW0+v+bdq0yeSaRkREuPa18LHVZXbv3t2cs8L1i0pzTo9Gv4Q1R1OX6+vra4qve/XqJb/++qvbfPoDQ/dLjRs3zq3I1HlujlWn72SOy9atW81xCQsLM/t3ySWXmBzYsqCfTd1XDexL8vvvv8tVV11lglgtrjz77LNl9OjRpsj/RI+JVg3Q119++aXbsh988EGTrtUEStr3wYMHu6WX9jyXZn7n9bVjxw558803pVmzZmZ/NXdb9yk/P19KS7d/zZo1snr1ard0PXf6Y6no/hWmnwfddv186OdEPy96jPXzU5KMjAx54oknpG7duubzqzm677///jG3T/d16NChJkda91VLKvQ4FL63HYvmVOqPQd1G3T49xnr9aPWasvp8AhWBnL4zhBadFJaVlSWXX365+TJu27atCQ41h2revHnmC/+tt94yuVZKf6Xrl/7y5ctNoKHvs9vt5oaoXxq33nqr+UI4lhkzZpgAUQMinT80NFS+++4786WtX0gapJ0KDZI0KNUvAw0K9Kb777//yqeffmr2Sb9sjreNR6Nf6p9//rn5wnnkkUdKdZz1y+rhhx82wcSAAQPMdukx07Q//vhD5syZUyy41cBGi8hbt25tvni0LlTh46NF9j/99JP06dNH7rnnHlO8XNpzeixatKlfXnputN7ivn37TBUBHdft1WU5A3MNSqZPn26CbR130vN7LCdzXHRdelxatmwpt99+uwnm9YeEnnM9//pD4lToD6P4+HjzGS/qvffeM8dF90sDv2rVqpm6ss8//7wJhnXQc3S8Y+IMCHV+zW10cgbUK1asMFUznNUDnOnO953MeT7Zz8Wjjz5qfsTo50yvf/0MaFCr16vud2nota8Bsub2aeDpNGXKFPMDSn+c6bSi9N6jx0nPc5MmTcw50OMzc+ZMU71CP0cPPfSQa34NSDVdq7Po9aOfLb1+dJ7Cx7Awva/p/ulydV81mNdzqMXOP/zwgyxdulQaNmx43P374osv5JxzzjEBrAaOe/bsMedPc5D1egJOCw5UeTt27HDoqerVq1exaS+88IKZ1rt3b7f0UaNGmfSnnnrKkZ+f70pPTk52dOzY0eHt7e3Yt2+fSVu3bp2Z95prrim2/MzMTEdKSoprfNq0aWZe/e+UlJTkCA4OdgQEBDiio6Nd6dnZ2Y4LLrjAzF+vXj235V544YUmvSSDBg0y03S/nRITEx1xcXHF5v3ll18cdrvdMXToULf0krbzaBYtWmTmPe+88xylsXXrVoenp6ejWrVqjt27d7sdM12WLnPGjBnFzqMOTz/9dLHlObdZ92fBggXFppfmnKoxY8aY+X/99Ve35Wzfvr3Ysvfv3++oVauW4+yzz3ZL1/fqMnRZJSlpHadyXF588UW35Y8ePdqkjx8/vsT1H217rr/+evNah5EjRzpuuukmh7+/v6Nhw4aONWvWuL3nn3/+Mdvbpk0bR2xsrNs0Xa8u75VXXjmhY6LnJSIiwtG8eXNXmi7TZrM5Lr74YvO++fPnu6bdeuutJq3wcSrteS7t/M7rq0GDBua8O8XExDhCQ0MdQUFBjqysrBM63rqcpk2bmtd9+vRxhIeHm/Os9L+OX3XVVWZc5yt6zU+fPt2k6f2g8Dp37drliIyMNOdl27Ztxa6Ryy+/3JGbm+tK13uY7mfR86L3oPr165t9Wr16tdu6//jjD4eHh4fZ7sL0XlX4fqX3Hj1/HTp0cFun0vGEhIQTOlZAVUDQdxpwfik2atTI9UX2yCOPOHr06GHSq1ev7ti4caNr/ry8PEdYWJiZv/CXgNPcuXPN+9566y23oK9///7H3ZaSginnjXv48OHF5tcba1kEfcfSunVrc2M/3nYezeeff27m1cCgNJ555hnzvpdeeqnYtD///NNM69mzZ7HzWKNGjRK/VJ3bfO211xabVtpzeqyg72j0/On8O3fuPKWg72SPiwYhup+FOaddd911J7QPzu0padAfJU8++aQjNTXV7T3333+/mf77778XW55uT1RUlPnCP9FjogGnTj9w4IAZ//LLL13HyMfHx/H444+75q1Tp44JRE/2PJ/M58J5fU2dOrXY/M5pek8obdA3Z84cM67XU+Hr6quvvjpq0KefA01bvnx5sWU///zzZpp+npyc97xVq1YVm3/IkCHFzotzmwovozD9XOmPLP3herSgT6fpMrp3717iMQZOJxTvnka0uEvr3BRWo0YNU1ym9e+ctLK01qXTOm9F51fObke0eFQ1b97cFFt89tlnpkHDNddcY4qttKhIi3mPx1mnResQFdWtW7diRaInS4uvJkyYYIprtF5W4bphp1p8fDK0rpIqXMRXeL+1vpHWcypKi4KOtb3a2KCo0p7TY9m+fbtphfzLL7+Yol0tHixMWyefbFH5qRyXkj5vderUMf8TExNLtQ36WXY25NDPie6n1pXUY6cteP/880/X51JbZqv58+fLwoULiy1Lu+05kePqpMWMs2fPNkV//fv3N/+1jp3W59Pia2eRrhbz6/XmrJd7Muf5VD4XHTp0KJZ2ssdbadGpFotrndF+/fqZ/zqu6cf6rPj7+5f4mXcW1xb+rOi9RovGCxchO+n9R4uTC3OeWz1OJXWxc/DgQVNkvHnzZunYsWOJ26hVVq688kr5/vvvzXpvuOEG89nWOqL62QBOJwR9pxGtl/Ljjz+6buZar+jxxx83dVy0rpA2wFBab0n9888/ZjgareOi9MtPAwC9KeqXlda7UlrfS+sCaSOHkho2FG5kofQGX5S+TxsrnCqt56ZfJLqPehy0ErV+WTgbPpxoheySaOCsNDAoDWddu5Lqmul2aXpJyzxe3bSSppf2nB6NBhr6Bavbrl+qWn9Nv9Q02NKgWut4FQ0CS+tkj0vRBjLKGZjl5eWd9PboMjSIHTNmjGzZssXU5dI6YzfffLPbsS1tPbajKVyvzxn0aXdKuh067dlnnzXHqKT6fKU9z6fyuSjr460BkHYppT/MlixZYurdaV27Y/3o0+OgDTJK4uwWyvl5ct5rjjb/sa4bPefHcrzrRu8/L7zwgqlDrPdD5/HT+n2arvci4HRA693TlAZk2uhAW5tqJXetRF30Zq59lf1XhF/iULhitQZmWuFbv4w3btxo+jLTSvj6Rfm///3vmNvi7MJFW78WpV8eJXXa6szRKanfLmcQWZgGpJpD5Oyb8OWXXzY5G870U6G/2DXnTSvuF/6COR7ncT506FCxaXp8Nb2kL9bjdVBb0vSTOaclef31103OkAbKmuOlX9DPPPOMOY7airMsnOxxqQhdunQx/7XyvZNzW/TcH+vYnihtAa8BiAZ1ek3o9eQM7PS/XhOaO+9s8Vw46CvteS6rz0VZ0VxLzTnTVq36v3AuZkl0+0u6bzhz4ZzzFL7XHK2D9JI+b873akvgYx0fbZRzLBrUPffccyaXXAfNUdRuaLS1duGGJkBVR9B3mtOgT4t23n33XVc3Glpcqzc7DWJK26ecBhz6fm1Fp0GBOl53IM6Wa/pFVpS2jCspsNMuOVTRHB/9oiipCwQt2tbt0pZ3hR04cMDchE+F3tC1KFC7gtBuGY5F98XZpYV24aJK6q5Ei6C1mwctsiwLp3JOC3N2b+NsoeukX3xa5FmUM4e3NDk/FXlcSsvZhVDhbkmcgaCzKPB4TuSYaPGf5qp+9NFHZrxnz57mvxbvaitxzVnXoFA/z3r9nux5LqvPRVnRgFePp17Xuq+6fcf7rGjreS2pKMr5+Sn8WdF7jebKFe0a5mj3H+e51ftQWWnQoIFpXa654lryUJrukoDKRtB3mtMvEC3i1Ru+FhspLU4ZNmyYKfLU3MCSvgw2bNjg+oWtwWLhPvGK/nI+Xk6aBhD6xaN1eLRujJOut3AOZNHcNVW0vz/tokH71CpKi+f0S7Twr3kNHnQ/y+LLTov2NPdU/2vfZSX1VbZu3TrzZe7MDdTuIvRY6zZrPTgn7fJCz4nSLlnKQmnP6dE46+otXry4WCff+v6iNLdXafcUJ6oij0tpAz5njlfhp9dotzi6vcOHD5fdu3cXe5/Wb3PWUzzRY+LMvdO+JXV+5w8jzVHWLmM0GNQfLEW7GSnteS6rz0VZ0vvAV199Vax+3dG6QlHawXzhbddjq58f3T9nMbzS7qCUFrEWDrrXr1/vCrCL3pu0bz5dlvbDWJSus+i1UJTmLJZ0bejnSatCnGpJA1CRqNN3BrjzzjvNl4v2lac5f9pDvhZ96q9hDWC0vy79ktM6d/oLXG+Qmpumv341TStKX3fddaaul/5S1zpuzr7btBj2eMUXWuSi69Evcg3mNNdM07SfPg1KS3pkm9aF0WJjLVbU9es2a26F3ly1qKXo00X0C1kHzRnQfr00x01zIjWHSr9QT7WDVK3Arn3jaSMWfRqHFoNefPHFrsewaU6EFglqcOusvK3brMdd60BqQxgt0tJK5lqUpBXH9QvH+di8slCac3o0d999twl8tDhQt1eL9TWHS5fbu3dvs9zCtMhXc6K0H0Ptm0yPk+YG67k42pNZKvq4lEQ7R3Y2XtDgQBtMaI6M1vHSPu308+6knftqTrkGT1pkp5X2dR9SUlJMLrJ+FvWzPXHixBM+Js5gTgMG7VeycAMVnab13QrPdyrnuSw+F2VJ7yE6nAgN4rTPRu2nTz8r2ujD2U+fnivNeS/ch54GiVqvTus2673giiuuMPNpwx19oo7ecwrT86OfBZ1P7yua46r9++n50kBZcwf1GjhWQx09jrouvc/oNuozz7XKim6zBo3H69sTqFIqu/kwTq2fPiftkkHn0X6/CvchNWnSJNPVgPajp91FnHXWWaaPq/fee8/VdcWePXscTzzxhKNr166mbzXt70rn0+4Mli5desJdoWjXDNq1ha5Hl6N958XHxxfrAsFJ+0vTvsu0/zTdvquvvtqxZcuWErts0a4SJk6c6GjZsqXD19fXdHuiXTQcPny4xO5fStNlS2FpaWmOCRMmmGU6+wnTvsu6detmupAo2o+b+uabb8z82heY7rt2IfPqq686cnJySjyPun8lOZFtPtFzeqwuW3Rc36/bq/t25ZVXmi4wjjb/smXLXPvn7P7EeW6O1S1MWR0XZz9up9Jli26Dfr7ffPPNYut3WrFihem2R/sr9PLyMue/ffv25trYtGnTCR8Tp9q1axfrLkUtWbLE9R5nty6ncp5LO/+xukQqbTc/hbtsOZ6SumxRej60H0T9fOh26zHVY6ufn6Ndo4899pg5vjp/ixYtHJMnTz5mVzp79+51PPDAA6YfSn2PHiPtS1HvUQsXLnSbt+j9SvvhGzt2rOlztGbNmub+qJ8RPbY//PDDCe07UFXY9E9lB54AAAAoX9TpAwAAsACCPgAAAAsg6AMAALAAgj4AAAALIOgDAACwAII+AAAACyDoAwAAsACCPgAAAAsg6AMAALAAgj4AAAALIOgDAACwAII+AAAACyDoAwAAsACCPgAAAAsg6AMAALAAgj4AAAALIOgDAACwAII+AAAACyDoAwAAsACCPgAAAAsg6AMAALAAgj4AAAALIOgDAACwAII+AAAACyDoAwAAsACCPgAAAAsg6AMAALAAgj4AAAALIOgDAACwAE+pYuZ5Na3sTUAF6Z0T7Xq9974bOO4WUOftWa7XO27vW6nbgorTYOpc1+vtt/Xh0FtEww+/q+xNQBHk9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAEAF+v333+Wqq66SWrVqic1mk6+//vq471m0aJG0b99efHx8pHHjxvLhhx+Wer0EfQAAABUoLS1N2rRpI++8884Jzb9jxw7p3bu39OjRQ9asWSMPPvigDB06VObPn1+q9XrKKcjOzjYb0qhRI/H0PKVFAQAAWMIVV1xhhhM1ceJEadCggbz66qtmvHnz5rJ48WJ5/fXXpVevXuWb05eeni5DhgwRf39/admypezevdukDx8+XF588cWTWSQAAMBpKysrS5KTk90GTSsLS5culUsuucQtTYM9TS+Nk8qeGzlypKxdu9aUL19++eWudN2gsWPHyhNPPHEyiwUAAKgQ87yalunyVj7ZX8aNG+eWNmbMGBMXnaqDBw9K9erV3dJ0XAPLjIwM8fPzK7+gTysczpw5U7p27WoqIDpprt+2bdtOZpEAAACnrZEjR8qIESPc0rTRRVVyUkFfTEyMVKtWrcSKiYWDQAAAACvw8fEptyCvRo0acujQIbc0HQ8ODj7hXL6TrtPXsWNHmTdvnmvcGeh98MEH0q1bt5NZJAAAAEqgsdXChQvd0hYsWFDqmOukcvpeeOEF0+pk48aNkpubK2+88YZ5vWTJEvntt99OZpEAAACWkJqaKlu3bnWNa08o2hVLeHi4nHXWWaaoeN++fTJjxgwz/e6775a3335bHnvsMbn99tvll19+kS+++MItA67ccvrOO+88s3Ea8LVu3Vp++uknU9yrrUg6dOhwMosEAACwhL/++kvatWtnBqV1AfX1008/bcYPHDjg6hlFaXctGuBp7p7276ddt2jpamm6a1En3bme9s33/vvvn+zbAQAALOmiiy4Sh8Nx1OklPW1D3/P333+f0npPKujTJsIl0bp9WonR29tbznTh53WUhg8PkZD2rcS3VjX56/p75NDchcd+zwWdpcUrT0hgi7Mlc88B2Tr+Pdk74yu3eeoNGyANRwwRnxpRkrzuX/nnwWclaeV613S7j7c0f/kJqXXjleZ1zE+LZcPwcZJ9OK7c9hXFBVzQS4Iu7isewaGSs2+XJMyaKjm7jmTVFxb1wFjxObtlsfSMDaslbuJ489q3TWcJPO8y8TqroXgEBMmh8Y9Kzr6d7m/w9JLQ6waKX4fuYvP0ksxNayRx5geSn5LEKaogQT2vlJDLrxWPkDDJ3rND4j6ZLNk7tpQ4b43Hnhe/Zq2LpaevXSmH3ni2WHrErcMkuMcVEvfZB5K8YK4r3fushhJ+w23i3aCxSH6+pK1aKvGfTxFHVmYZ7x1KEnxxbwm54rqCc757h8R9PEmydmwucd6aT4w/6jk/+HpBVx5RQx+UoPPc+1tLX79KDr46xi3Nr01HCevbX7zr1hdHTo5kRq+XQ28+z0lCxQd9oaGhx2ylW6dOHbnttttM/zR2+5n5pDePAH9JXhctez6cLR2/PP5jVPzq15FOcyfJ7smfy5qBj0hEz27SetJzknkgRmIXLDbz1LzhCmn+8kjZcO8YSVyxVhrcP0i6zJsii1peLtkx8WaeFq+OkmpXXCirb3pQcpJTpNUbT0mHWW/L0gv7l/s+479z2f5cCb12kCTMnCzZO7dKYI/eEnXvk3LwmQckP7X4D6LY918Rm8eRS80eECjVR74iGX8f6VTT7u0rWdv+lfTVSyT85mElHurQ628T35btJX7Ka5KfkS6hNw6RiKGPSMzrT3FqKkBAp/Mkot8Qif3oXcnavlmCL+0rNUaMk72jhpUYeB9+Z7z7eQ8Mktrj3pS0v/4sNq9/+67i06ip5Ca4/3jzCA2XGo88K2krF0vcJ5PE5usnEf3vkKghD8jhd18qpz2FU0Dn8yXipqESM/0dydoeLSGXXS01HnlG9jxxV4nn/NBbz4ut0NOp7AHBUufZtyR1ZcE93il93V8SM2WCa1yDusICOp4rkbcNl/jZMyRz41oRDw/xrl2PE1PGbF7W623kpCIyzXbUhwSPGjXK9Nmng76uXbu2vPfee3LnnXfKm2++eUY/nSNm/u+yecwEOfTNzyc0f707b5KMHXtl02MvSeq/22XXu5/IwdnzpcEDt7nmafDgYNkz5QvZO32OpG7aJuvvGSN56ZlS97brzXTP4ECpO/h62fjoixK3aJkkr/5H1g4dJeHntpfQLm3KbV/hLqhnH0lbslDSly2S3IN7JfHzyeLIzpaAbj1LPFSO9FTJT0l0Db7NzhFHdpZb0Je+8ndJ+fFLyYo+kqtbmM3X3yw/ac50ydq8QXL2bJeEj98Rn0bNxLv+2ZyiChDc62pJ+f0nSV28UHL275G4Ge+a8xh0vnuujVN+WqrkJSe6Br+W7cz8aSv/LBbYRQy4U2ImvyqOvFy3af5tOonk5UncxxMl5+A+8yMjdsa7EtCxu3hWq1mu+wuRkF7XSPJv8yV18c/mnMdOf6fgnF9w6dHPeVKia/Br1bbgnK9wD/ocuTlu8+Wnpx2ZaLebz0P8F1Ml5dcfJOfQfrNuDfyBSsnpmz59uqlEeOONN7rSrrrqKtOoY9KkSaZZsbY+ef75500wCJHQrm0l9hf3x6XELFhscu6UzctLQtq3lG0vTToyg8Mhsb8skdCuBRU9tSjZ7u0tsQuXuGZJi94u6bv2SVjXtpK4fC2Hurx5eIpX3YaS/FOhYnmHQzKj14l3gyYntIiAcy82OXr6ZXCitIhPcxB0PU65h/ZLbnyMWW/2zpKLGFFGPDzFp15jSZr35ZE0h0MyNq41gfeJ0OAwdcUf7ufdZpOoO0ZI0o9fmS/2ovScO/JyzLpcq83JNv99z24uqYcPnNJu4TjnvH5jSZw3y/2c/7NGfBs1kxOpVBF8/mWSuvz3Yte6b7PWUu/NjyUvLVUyN62T+NkfSX5aipmmnzPP8Ehx5Duk9rg3/itW3i5xM6eZqiRAhef0adcszhYnhWma8zlw2sK3cMsTq/OpHilZh2Ld0nTcKyRI7L4+4h0ZJnZPT8kqUjcv61Cc+NSILFhGjUjJy8qW3KSCm4OT1ufzqR5VAXsBLaKzeXgUK9rJT04y9fuOx6teY/GqdZbJKSwNe3CoKQJyZKSf1HpxajyCgs151xy7wnTcI+T4x9+7wdniXae+ySksLOSK601OXvLP35b4voxN68QjOMzUI9QgxO4fIOH/N7Bgm0LCT2mfcILnPKmkcx523MPn06CJqY+X8pv7OU9fv1piJr8m+//3pMTP+lB8m7aSGg+PE7EVfB17Vqth/oddM0ASv51p6gJqDmKtJ14wVUOACg/66tatK1OmTCmWrmk6TcXFxUlYWFiFP5QYqMq0iDZ7366jNvrAmSno/Esle89Ot0Yf3vUaSfClV0nM1DeO+j7N/dO6X8G9rpH6E2fJWa/PkJyYQ5KblCDiyK+grcfJ0CLgrD07ijX6SFv+u6SvWSE5e3dJ+uplcnDCOPFt2MTk/hn/1ZfXgC/tryWSvWubHNb6f46CeqVAhRfvvvLKK3LDDTfIDz/8IJ06dXL1OfPvv//Kl18WFH+sXLlS+vXrV+L7x48fX24PJa6qNFdPc/sK0/GcpBTJz8yS7NgEyc/NFZ9qEUXmiZCsgwU5hPrfw8dbPEOC3HL7vKtFSNahmAraE2vLT00RR16e2INC3NLtwSHFcoGKsnn7iH+H7pI8b2bp15ucaKoA2Pz83XL7TmS9OHV5KcnmvBfNVdXxojlBJZ33wM7nS8LXn7ql+zZpKR5BIVL35SM/oDVnKbzfYBMM7n3sDleQoIPJ7dUWuw6HhPS62gR/qIBzHlLSOU84/jnvcoHEf/XJcdeTG3NI8pKTxKt6TcnctFbyEgsa7WUXLu7PzZWcmIPiGUGJDioh6Ovbt68J8CZPnizR0dEmTZ/QoQ066tevb8aHDSu5BeLp8lDispa4bI1EXXGBW1rkxedKwrI15rUW3SWt/kcie3Y70vWLzSYRPbrJrnc/NqNJqzdIfna2mefgVwVFBgFNGoh/vdqu5aCc5eWaRhS+TVtL5rqVrvPk06S1pP3+4zHf6teum6mjpY02Skvr9Dhyc816M9YsN2me1WqJZ3iUZB+l+wiUobxcydq1VXybt5H0v5e7zrtf83Mk+Zdj94gf0Km7iJeXpC5d5JaeuuRXydjoft1qa+DUpb+axiIlBf4q8LxLCrrw+IdrvtzP+c6t4teijcmRc53zFm0kaeF3x3xrQOfzCs75kl+PuxqPsAhTbcQZ7Ok683OyxbtmbcnasvG/mTzEM7Ka5MYeLoMdg5Pd03qtd0+6c2btHVpz7KraQ4krssuWgMZnucb9G9SR4DbNJDs+yfTB1/S5EeJbu7qsHfy4mb5r8udS756bpdn4R003L5E9upouWlb2vcu1jB0TpkmbqS9J4qoNkrRyndS/f5B4BvjJnulzzPTc5FTZM2226acvJz5JclJSpdWE0ZKwdDWNOCpQyi/fSfit90r27m2uLlvsPj6StqzgBh92632SlxQvyXM/LVa0m7FupamfU5TNP1A8wyJddYU8q9cy/zUXT1v8OjLTJW3pLxJy3SDz/vzMDAm94XbTjQSNOCpG8vxvJHLog+aca5Gddtli8/GVlP8CNJ2WlxAvCbMLHptUuGhXgwZnRX0nHS+apq13NedQW+q63t+zt2Rt3ST5WZni17KthN8wWBJmT5f8jEItPlEukuZ/LVF3PCRZO7aYbnq0yxY956l/FPTaoI1wtJudhC+nu70v6PzLSjzn+t6wa/qbYlvNLfSMqikR/QZLzuEDkr5hdcFnIDPDtNoNu+ZmyY2Lldy4w6afQEULXlRY0Ldu3ZFWg8dzzjnnyJkupEMr6bbwI9d4i1cKWuHumTFH1g0ZKT41o8Sv7pEuFTJ27jUBXotXR0r94QMlc+9BWX/XaFcfferArB/EOypcmoy5v6Bz5rWbZEWfoW4dL298+AVpnp8v7b9403TOHPtf58yoOBmrl0hiYLAE9+4nHkHaOfNOiX3neVfjDm15V7i1pUmrVkt8GjeXmLeLd8qr/Fp3NIGkU8TtD5n/yd9/IcnfF7QeTJz9oYQ68k3ffKKNfjatlYSZH5TjnqIw/cLVYn2tYK/Bedae7XLo9bGuHDjNdZV89/PuVaO2KcY98ErBo5VOhk/Ds02gYPfxk+yDeyVuxjvFcg1RPtJW/GGK4MOuvUU89Zzv3i4HX33aVaXCFLcWqVup59yvaUs58PLo4gvMzxfvOg0kqPvFplFObmK8ZGz4WxLmfGyKcJ3iZk41RctRd44Qu7ePZG6LlgMvPenetQtwEmyOYz0HpBDtZFk7ZNbZC3fM7Hx74bS8vDw5WfO8mp70e3F66Z1TUDVA7b3vhkrdFlSMOm8f6f5ix+19OewW0WDqkSeMbL+tT6VuCypOww+PXQxe2X4Mbl6my7s8eZOcMa13d+zYIdu3bzf/Z8+ebYp33333XVmzZo0Z9LU+j1enAQAA4DQt3q1X78gjYLTlrj5x48orr3Qr0tXuWp566im55ppryn5LAQAAULH99K1fv97k9BWlaRs3/tfaCAAAAKd30Ne8eXPTcjc7u+BxQEpfa5pOAwAAqMpsXvYyHc7YLlsmTpxonrVbp04dV0tdbd2rjTm+/bbkxwkBAADgNAv6OnfubBp1fPLJJ6aTZqVP3xgwYIAEBASU9TYCAACgsjpn1uDuzjvvPNX1AwAAoCoFfXPnzjWPWvPy8jKvj/eYNgAAAJyGQZ92w3Lw4EGpVq3aMbtk0Xp9p9I5MwAAAMreCTc3yc/PNwFfTk6OXHTRRaYun6YVHQj4AABAVWf3tJXpcDoodRtjLd7Vfvr0sWwAAAA4PZxU5HbLLbfIBx/woHcAAIAzuvVubm6uTJ06VX7++Wfp0KFDsW5aXnvttbLaPgAAAFRW0LdhwwZp3769eb158+ZiDTkAAABwBgR9v/76a9lvCQAAAKpe58wAAACnK5uX9UomaYILAABgAQR9AAAAFkDQBwAAYAEEfQAAABZA0AcAAGABtN4FAACWYz9NnpdblsjpAwAAsACCPgAAAAsg6AMAALAAgj4AAAALIOgDAACwAII+AAAAC6DLFgAAYDk2L7psAQAAwBmI4l0AAAALIOgDAACwAII+AAAACyDoAwAAsABa7wIAAMuxe9J6FwAAAGcgincBAAAsgKAPAADAAgj6AAAALICgDwAAwAJovQsAACzH5kHrXQAAAJyBKN4FAACwAII+AAAACyDoAwAAsACCPgAAAAsg6AMAALAAumwBAACWY6fLFgAAAJyJKN4FAACwAII+AAAACyDoAwAAsACCPgAAAAug9S4AALAcm90mVkNOHwAAgAUQ9AEAAFgAQR8AAIAF2BwOh6OyNwIAAKAi/dmuQ5kur/vfq6SqI6cPAADAAgj6AACA5dg87GU6lNY777wj9evXF19fX+nSpYusWLHiqPPm5OTIM888I40aNTLzt2nTRn788cfTv8uWvffdUNmbgApS5+1ZrtfzvJpy3C2gd0606/Xe4TdW6rag4tR56wvX67RJT3LoLSLgrucrexOqrJkzZ8qIESNk4sSJJuCbMGGC9OrVS6Kjo6VatWrF5h89erR8/PHH8v7770uzZs1k/vz5cu2118qSJUukXbt2J7xecvoAAAAq0GuvvSZ33HGHDB48WFq0aGGCP39/f5k6dWqJ83/00UcyatQoufLKK6Vhw4YybNgw8/rVV18t1XoJ+gAAACpIdna2rFq1Si655BJXmt1uN+NLly4t8T1ZWVmmWLcwPz8/Wbx4canWTdAHAABwijQwS05Odhs0rajY2FjJy8uT6tWru6Xr+MGDB0tcthb9au7gli1bJD8/XxYsWCBz5syRAwcOlGobCfoAAABO0fjx4yUkJMRt0LSy8MYbb8jZZ59t6vN5e3vLfffdZ4qGNYewNAj6AAAATtHIkSMlKSnJbdC0oiIjI8XDw0MOHTrklq7jNWrUKHHZUVFR8vXXX0taWprs2rVL/v33XwkMDDT1+0qDoA8AAFiO3cNWpoOPj48EBwe7DZpWlObUdejQQRYuXOhK0yJbHe/Wrdsxt1nr9dWuXVtyc3Nl9uzZcvXVV5/eXbYAAACcyUaMGCGDBg2Sjh07SufOnU2XLZqLp0W2auDAgSa4cxYPL1++XPbt2ydt27Y1/8eOHWsCxccee6xU6yXoAwAAqED9+vWTmJgYefrpp03jDQ3mtLNlZ+OO3bt3u9XXy8zMNH31bd++3RTranct2o1LaGhoqdZL0AcAAFDBtDGGDiVZtGiR2/iFF14oGzduPOV1UqcPAADAAgj6AAAALIDiXQAAYDk2u02shpw+AAAACyDoAwAAsACCPgAAAAsg6AMAALAAgj4AAAALoPUuAACwHLsHrXcBAABwBqJ4FwAAwAII+gAAACyAoA8AAMACCPoAAAAsgNa7AADAcmy03gUAAMCZiOJdAAAACyDoAwAAsACCPgAAAAsg6AMAALAAgj4AAAALoMsWAABgOTa79fK9rLfHAAAAFkTQBwAAYAEEfQAAABZA0AcAAGABBH0AAAAWQOtdAABgOTa7TayGnD4AAAALIOgDAACwAII+AAAACyDoAwAAsACCPgAAAAug9S4AALAcuwetdwEAAHAGongXAADAAgj6AAAALICgDwAAwAII+gAAACzgpIK++vXryzPPPCO7d+8u+y0CAABA1Qj6HnzwQZkzZ440bNhQLr30Uvn8888lKyur7LcOAACgHNjstjIdzuigb82aNbJixQpp3ry5DB8+XGrWrCn33XefrF69uuy3EgAAAJVXp699+/by5ptvyv79+2XMmDHywQcfSKdOnaRt27YydepUcTgcp7Z1AAAAqPwncuTk5MhXX30l06ZNkwULFkjXrl1lyJAhsnfvXhk1apT8/PPP8umnn5bNlgIAAKBigz4twtVA77PPPhO73S4DBw6U119/XZo1a+aa59prrzW5fgAAADhNgz4N5rQBx3vvvSfXXHONeHl5FZunQYMGctNNN8mZLuCCXhJ0cV/xCA6VnH27JGHWVMnZtbXEeaMeGCs+Z7cslp6xYbXETRxvXvu26SyB510mXmc1FI+AIDk0/lHJ2bfT/Q2eXhJ63UDx69BdbJ5ekrlpjSTO/EDyU5LKZyfhEn5eR2n48BAJad9KfGtVk7+uv0cOzV14zCMUfkFnafHKExLY4mzJ3HNAto5/T/bO+MptnnrDBkjDEUPEp0aUJK/7V/558FlJWrneNd3u4y3NX35Cat14pXkd89Ni2TB8nGQfjuPsVJCA8/Vav+rItf6lXuvbjjq/zc9fQvr0F782ncXuHyi5CTGSNHu6ZG78u2C6j68E9+5npnsEhkj23h2SOPtDydl9ZJk2bx8Jufpm8W3dydwPcuMOS+pvP0janwsqZJ8hMnPNVpnx12aJS8uUJlEh8liPdtKqZniJh2buPztl7Py/3NK8Peyy7IHrXONa7Wniko3y1YYdkpKZLW1qR8qoi9vJWWFBrnl6f/C9HEhOd1vO8PNayeDORzJWgAoL+rZv3y716tU75jwBAQEmN/BM5tf+XAm9dpAkzJws2Tu3SmCP3hJ175Ny8JkHJD81udj8se+/IjaPI4fcHhAo1Ue+Ihl/Lz2S5u0rWdv+lfTVSyT85mElrjf0+tvEt2V7iZ/ymuRnpEvojUMkYugjEvP6U+W0p3DyCPCX5HXRsufD2dLxy3eOe2D86teRTnMnye7Jn8uagY9IRM9u0nrSc5J5IEZiFyw289S84Qpp/vJI2XDvGElcsVYa3D9IusybIotaXi7ZMfFmnhavjpJqV1woq296UHKSU6TVG09Jh1lvy9IL+3NyKoBf+24Seu1ASZj5vmTv2iKBF/WWqHuelIPPPljitS4eHhJ172jJS02WuCmvSV5SvHiER4oj48gXediAu8WrZl2Jn/G2mR7Q6QKJuu8pOfj8Q5KflGDmCblukPg2aSUJM96S3PgY8W12joTeONTMn7lhFee+nM2P3iOv/bZORl3cXlrXDJdPVm+Re+f8IV8N7iXh/r4lvifQ21PmDL7cNV60Tef0ldHy2Zqt8kyvTlIrxF/eW/KP3DtnsXw56DLx8fRwzTfs3BZybeuGrvEA71OqjYUS2OzW66r4pPb4eAGfVQT17CNpSxZK+rJFkntwryR+Plkc2dkS0K1nifM70lMlPyXRNegN3JGd5Rb0pa/8XVJ+/FKyoo/k8hRm8/U3y0+aM12yNm+QnD3bJeHjd8SnUTPxrn92ue0rCsTM/102j5kgh775+YQOSb07b5KMHXtl02MvSeq/22XXu5/IwdnzpcEDt7nmafDgYNkz5QvZO32OpG7aJuvvGSN56ZlS97brzXTP4ECpO/h62fjoixK3aJkkr/5H1g4dJeHntpfQLm04NRUgqEcfSVu6UNKX67W+TxJnvv/ftd6jxPkDuvY0uXtxk1+W7B3RkhcfI9lbN5kcQsPLS/zadJGkbz6W7G2bJC/2kCT/MEtyYw6anH4nnwZNJG35b5K1daNZht5vdBne9Rpz3ivAJ6s2y7WtGsjVrepLw4hgefKS9uLr6SHfbChS+lKYzSaRAb6uISLA1y2X79O/t8rQLs3kosa1pElUqDxzeWeJSc2QRVv3uy3G39vLbTl+XgR9OHUn/CkKCwsTm+3E+qGJjy/InTijeXiKV92GkvxToWI6h0Myo9eJd4MmJ7SIgHMvNjl6GvidKO+zGorN09Osxyn30H6TC6Drzd65pXT7gXIV2rWtxP5yJKhXMQsWm5w7ZfPykpD2LWXbS5OOzOBwSOwvSyS0azszqkXJdm9viV24xDVLWvR2Sd+1T8K6tpXE5Ws5i+XJw6PgWl/wtds5yoxeL971S77WfVt3kKydW0wuvF/rjiY3MH3Vn5Kiy3A4xGb3EJuHhzhyctze58jJNj/gnLJ2bBa/1h0kbdkvJvdPq4d4VqspmXOml9/+wsjJy5dNhxLdilTtNpt0qVdd1h04erWKjOxcufL9702A16x6qNzXvZU0igwx0/YlpUlsWqZ0Oau6a/4gHy9pVSPcLLNXs7qu9A9X/CsfLNskNYL85fJmdeXmDmeLpwVzplBJQd+ECRPKeNWnN3tgkLlpF61Hl5+cJF7Vax/3/V71GotXrbMk/pP3Srfe4FDzRVG4mMi5Xq1rhKrFp3qkZB2KdUvTca+QILH7+ohXWIjYPT0lq0jdvKxDcRLQtKBox6dGpORlZUtuUorbPFqfz6d6VAXshbXZA4ILrvXkRLd0za33ql6rxPd4RlYXz/AoSf9rscROHC+eUTVMsawGkCk/fCmOrEzJ2h4twZdfL3EH95ll+Xc4z/xw09w+p8Qvp0rYTXdJrecmiSMvVyTfIQmfTzK5gyhfiRlZkudwFCvGDff3kZ3xySXn7IcFyZheHeXsyBBJzcqRGas2y+DPf5VZgy6T6kH+Epee6VpGYZobqMGgU/92jaVZtVAJ9vWWdfvj5K3FG8z0hy8iZx8VFPQNGjRIyoo+vaPoEzx8fHzMYBVaRJu9b9dRG30AOI3ZbJKXkiwJn00yOXs5e3aIR0i4afSlQZ+K/+htCR8wTGo9rwFdnuTs3WFyA73rNnAtJvCCK0y1jdhJL5niXe/GzSX0hiGSl5Rw1CogqDxtakWYwemcWhFy/YfzZfa67XJP91YnvJxbOhzJQdYiYE8Pu7zw82rTmMO7UL0/oLROupJAXl6efP3117JpU8EvzpYtW0rfvn3Fw+P4H8jx48fLuHHj3NK0c+exY8fK6SI/NcXcqO1BBdn2TvbgEMkrkiNQlLbI8+/QXZLnzSz9epMTTZGgtgwsnNt3IutFxdNcPc3tK0zHc5JSJD8zS7JjEyQ/N1d8qkUUmSdCsg4W5BDqfw8fb/EMCXLL7fOuFiFZh2IqaE+sKz8tueBaL5KTbg8KPeo1l5+UKI78XBPwOeUe2iceIWEmt0/y8kw9vpg3x5r7gc3Xz1zb4YMfNC10DS36v6q/xH3wsmT+U9DiN2f/bvGuXV+Cel5F0FfOQv18xMNmk/j/cuec4tOz3OrpHYuXh93k2O1JTDPjEf/lGuoyogL9XPNpy+Cm1Y5eUtO6Rrjk5jtkf3K61A8/0soXKK2TqiCwdetW8/g17Z9Pn8Grwy233GICv23bjt6FgdPIkSMlKSnJbdC000permlE4du09ZE0m018mrSW7B2bj/lWv3bdTL08bbRRWtm7t4sjN9dtvZ7VapmipOOtFxUvcdkaiejZ1S0t8uJzJWHZGvNai+qTVv8jkT27HZnBZpOIHt0kcVnBF33S6g2Sn53tNk9AkwbiX6+2azkoR5oLp9d6k1ZFrvVWkr2z5Gsua0e0eEbWMPM5eUbVNK1udXmFaZ1e82POL0B8m7WRzHUrC1bh4WnuE4UDRzN/fr7bclE+NGBrXj1UVuw+fCSYdzjM+Dk13X+kHU1evkO2xiabhhiqdkiAeV14mVoMvOFg/DGXGR2TJPpo16LFwjg1Ngs+e/ekcvruv/9+adSokSxbtkzCwwv6K4qLizOBn06bN2/eMd9/phTlpvzynYTfeq9k797m6rLF7uMjact+NdPDbr3P3OST535arGg3Y91KyU9LLbZMm3+geIZFFuQI6An6r86Q5ihovR9HZrqkLf3FdOWg78/PzJDQG2439YNoxFExXbYEND7LNe7foI4Et2km2fFJpg++ps+NEN/a1WXt4MfN9F2TP5d699wszcY/arp5iezR1XTRsrLvXa5l7JgwTdpMfUkSV22QpJXrpP79g8QzwE/2TJ9jpucmp8qeabNNP3058UmSk5IqrSaMloSlq2nEUUFSfv1Owm/Ra327ZO/aKoEXXfnftb7ITA+79V7JS4yX5G8/M+Npf/wkgef3Mt0rpf72o3hWqyFBl11r+thz8mnWxvTnkXt4vwkQQ665VXIO7XMt05GZIVlb/pGQq28xLYW1nz+fxi0koPOFkvgVDTkqws0dmsiYH1dKi+ph0rJGuHy6eotk5ORK35b1zfSnflgh1QL9ZPj5BT/CJy/daLp2qRsaKClap++vzXIgOU2ubV1QZK+NIQe0aywfLN8kZ4UFSq3gANNli+b6aWtetXZ/nAkCO9WNEn8vT1l3IF5eXbRWrmxez9TxAyo86Pvtt9/cAj4VEREhL774onTv3l2sImP1EkkMDDYdrHoEaYetOyX2neddjTs8wyOL/UrXXDmfxs0l5u1nS1ymtvTTQNIp4vaHzP/k77+Q5O9nmdfagWuoI9/0zSfaCGDTWkmY+UE57imcQjq0km4LP3KNt3iloBXunhlzZN2QkeJTM0r86tZ0Tc/YudcEeC1eHSn1hw+UzL0HZf1do1199KkDs34Q76hwaTLm/oLOmddukhV9hrp1vLzx4RekeX6+tP/iTdM5c+x/nTOjYmSsXvrftX7jkWv93ReOXOth7td6XmKcxL77vPlxVn3kyyYg1IDPtN79j107b76qv3iERkh+eqpkrF0uSRo05h/JCYybNkFC+g6Q8EH3H+ng+bvPJG0xnTNXhF5N60pCepa8t2SjaYTRNCpE3r7uPFfx7sGUdNOi1yk5K1ueXbDazBvs4yXNq4fJtP49THcvToM6NZWMnDx5bsEqExi2rR1plunso087c57/7x6ZtHSj5OTmSa2QANNy95b2dMmFU2dzaLvyUtJg77vvvpNzzz3XLf3PP/+Uq6666pS6bNl73w0n/V6cXuq8XRDEqnleTSt1W1AxeudEu17vHX4jh90i6rz1het12qQnK3VbUHEC7nq+Sh/u9X1K7mfzZLX+rqCU74yr09enTx+58847Zfny5aYvIh005+/uu+82jTkAAABwBgR9b775pqnT161bN/H19TWD5vo1btxY3njjjbLfSgAAAFR8nb7Q0FD55ptvTCvejRs3mrQWLVqYoA8AAKCqs3ucHi1uq0Q/fVOmTJHXX39dtmwpeOzX2WefLQ8++KAMHTq0LLcPAAAAlRX0Pf300/Laa6/J8OHDTRGvWrp0qTz00EOye/dueeaZZ8pi2wAAAFCZQd97770n77//vvTv39+Vpg04zjnnHBMIEvQBAACcAQ05cnJypGPHjsXSO3ToILm5uWWxXQAAAKjsoO/WW281uX1FTZ48WW6++eay2C4AAABUlYYcP/30k3TtWvBcUe2zT+vz6fN4R4wY4ZpP6/4BAADgNAz6NmzYIO3btzevt23bZv5HRkaaQac56XMGAQAAqhqb3XoxykkFfb/+WvUfNQIAAIBTrNMHAACA0wtBHwAAgAUQ9AEAAFgAQR8AAIAFEPQBAADLsdntZTqU1jvvvCP169cXX19f6dKli6xYseKY80+YMEGaNm0qfn5+UrduXfPo28zMzFKtk6APAACgAs2cOdP0aTxmzBhZvXq1tGnTRnr16iWHDx8ucf5PP/1UnnjiCTP/pk2bTF/JuoxRo0aVar0EfQAAABVIH1xxxx13yODBg6VFixYyceJE8ff3l6lTp5Y4/5IlS6R79+4yYMAAkzt42WWXSf/+/Y+bO1gUQR8AAMApysrKkuTkZLdB04rKzs6WVatWySWXXOJKs9vtZnzp0qUlLvvcc88173EGedu3b5fvv/9errzyylJtI0EfAADAKRo/fryEhIS4DZpWVGxsrOTl5Un16tXd0nX84MGDJS5bc/ieeeYZOe+888TLy0saNWokF110EcW7AAAAFW3kyJGSlJTkNmhaWVi0aJG88MIL8u6775o6gHPmzJF58+bJs88+W/6PYQMAADid2cr42bs+Pj5mOJ7IyEjx8PCQQ4cOuaXreI0aNUp8z1NPPSW33nqrDB061Iy3bt1a0tLS5M4775Qnn3zSFA+fCIp3AQAAKoi3t7d06NBBFi5c6ErLz8834926dSvxPenp6cUCOw0clcPhOOF1k9MHAABQgbS7lkGDBknHjh2lc+fOpg8+zbnT1rxq4MCBUrt2bVedwKuuusq0+G3Xrp3p02/r1q0m90/TncHfiSDoAwAAqED9+vWTmJgYefrpp03jjbZt28qPP/7oatyxe/dut5y90aNHi81mM//37dsnUVFRJuB7/vnnS7Vegj4AAIAKdt9995nhaA03CvP09DQdM+twKqjTBwAAYAEEfQAAABZA8S4AALAcWxl32XI6IKcPAADAAgj6AAAALICgDwAAwAII+gAAACyAoA8AAMACaL0LAAAsx0brXQAAAJyJKN4FAACwAII+AAAACyDoAwAAsACCPgAAAAug9S4AALAcm916+V7W22MAAAALIugDAACwAII+AAAACyDoAwAAsACCPgAAAAsg6AMAALAAumwBAACWY/ewidWQ0wcAAGABBH0AAAAWQNAHAABgAQR9AAAAFkDQBwAAYAG03gUAAJZjs9N6FwAAAGcgincBAAAsgKAPAADAAmwOh8NR2RsBAABQkbbf1qdMl9fww++kqiOnDwAAwAJovQsAACzHZrdevleVC/p23N63sjcBFaTB1Lmu13uH38hxt4A6b33hej3Pq2mlbgsqTu+caNfrwyMHcugtotr4GZW9CSjCemEuAACABRH0AQAAWABBHwAAgAUQ9AEAAFhAlWvIAQAAUN5sPHsXAAAAZyKKdwEAACyAoA8AAMACCPoAAAAsgKAPAADAAgj6AAAALIAuWwAAgOXY6LIFAAAAZyKKdwEAACyAoA8AAMACCPoAAAAsgKAPAADAAmi9CwAALMdmt16+l/X2GAAAwIII+gAAACyAoA8AAMACCPoAAAAsgKAPAADAAmi9CwAALMfGs3cBAABwJqJ4FwAAwAII+gAAACyAoA8AAMACCPoAAAAsgKAPAADAAgj6AAAALIB++gAAgOXY7NbL97LeHgMAAFgQQR8AAIAFEPQBAABYAEEfAABABXvnnXekfv364uvrK126dJEVK1Ycdd6LLrpIbDZbsaF3796lWidBHwAAQAWaOXOmjBgxQsaMGSOrV6+WNm3aSK9eveTw4cMlzj9nzhw5cOCAa9iwYYN4eHjIDTfcUKr1EvQBAADrsdnKdiiF1157Te644w4ZPHiwtGjRQiZOnCj+/v4yderUEucPDw+XGjVquIYFCxaY+Qn6AAAAKlhWVpYkJye7DZpWVHZ2tqxatUouueQSV5rdbjfjS5cuPaF1TZkyRW666SYJCAgo1TaS0wcAAHCKxo8fLyEhIW6DphUVGxsreXl5Ur16dbd0HT948OBx16N1/7R4d+jQoaXeRjpnBgAAOEUjR4409fQK8/HxkbKmuXytW7eWzp07l/q9BH0AAACnSAO8EwnyIiMjTSOMQ4cOuaXruNbXO5a0tDT5/PPP5ZlnnjmpbaR4FwAAoIJ4e3tLhw4dZOHCha60/Px8M96tW7djvnfWrFmmnuAtt9xyUusmpw8AAKACaTHwoEGDpGPHjqaYdsKECSYXT1vzqoEDB0rt2rWL1QnUot1rrrlGIiIiTmq9BH0AAMBybPbSdbNSlvr16ycxMTHy9NNPm8Ybbdu2lR9//NHVuGP37t2mRW9h0dHRsnjxYvnpp59Oer0EfQAAABXsvvvuM0NJFi1aVCytadOm4nA4Tmmd1OkDAACwAII+AAAACziloG/Pnj1mAAAAwBkW9OXm5spTTz1lepquX7++GfT16NGjJScnp3y2EgAAAKek1A05hg8fLnPmzJH//e9/rv5k9FlxY8eOlbi4OHnvvfdObYsAAADKma1I61grKHXQ9+mnn5reoK+44gpX2jnnnCN169aV/v37E/QBAABUQaUOc/URI1qkW1SDBg1ML9MAAAA4A3L6tE+ZZ599VqZNm+Z6xpw+EuT5558/an8zZ7KgnldKyOXXikdImGTv2SFxn0yW7B1bSpy3xmPPi1+z1sXS09eulENvPFssPeLWYRLc4wqJ++wDSV4w15XufVZDCb/hNvFu0Fif3SJpq5ZK/OdTxJGVWcZ7h6MJOL+XBF18lXgEh0rOvl2S8OVUydm17ajz2/z8JaRPf/Fr01ns/oGSmxAjSbOnS+bGvwum+/hKcO9+ZrpHYIhk790hibM/lJzdR5Zp8/aRkKtvFt/WncQjIEhy4w5L6m8/SNqfCzhR5Sz8vI7S8OEhEtK+lfjWqiZ/XX+PHJq78NjvuaCztHjlCQlscbZk7jkgW8e/J3tnfOU2T71hA6ThiCHiUyNKktf9K/88+KwkrVzvmm738ZbmLz8htW680ryO+WmxbBg+TrIPx5XbvsKdX9eLxf+CK8UeGCK5B/dIytyPJHfv9qMeJpuvvwRc9n/i07Kj2P0DJC8xTlK/+1iyo9cVm9f/wj4SePmNkv7nfEn97hNXukd4NQm88ibxqtdExNNLsjevk5RvPxJHajKnB+Uf9F133XVu4z///LPUqVNH2rRpY8bXrl0r2dnZcvHFF4uVBHQ6TyL6DZHYj96VrO2bJfjSvlJjxDjZO2qY5KckFZv/8DvjxeZx5JDbA4Ok9rg3Je2vP4vN69++q/g0aiq5Ce43d4/QcKnxyLOStnKxxH0ySWy+fhLR/w6JGvKAHH73pXLaUxTm176bhF47UBJmvi/Zu7ZI4EW9JeqeJ+Xgsw9Kfkk3ZQ8Pibp3tOSlJkvclNckLylePMIjxZGR7polbMDd4lWzrsTPeNtMD+h0gUTd95QcfP4hyU9KMPOEXDdIfJu0koQZb0lufIz4NjtHQm8caubP3LCKk1SOPAL8JXldtOz5cLZ0/PKd487vV7+OdJo7SXZP/lzWDHxEInp2k9aTnpPMAzESu2CxmafmDVdI85dHyoZ7x0jiirXS4P5B0mXeFFnU8nLJjok387R4dZRUu+JCWX3Tg5KTnCKt3nhKOsx6W5Ze2J/zXQF8WneRwN4DJOXrDyVnzzbx795LQm9/VOJefUwcaSnF3+DhIaFDHjP3geRP35K8pATxCItwu9adPOs0EL/OPSTnwG73CV7eZh05B/ZIwgcvmqTAS6+X0IEPScJ7z4icYue8sLYTKt7V1rmFh+uvv1769Olj6vHpoK81MNRpVhLc62pJ+f0nSV28UHL275G4Ge+KIztLgs6/pMT589NSJS850TX4tWxn5k9b+WexwC5iwJ0SM/lVceTluk3zb9NJJC9P4j6eKDkH90n2zq0SO+NdCejYXTyr1SzX/UWBoB59JG3pQklfvkhyD+6TxJnviyM7WwK69SjxEAV07Wly9+ImvyzZO6IlLz5GsrduMjmEhpeX+LXpIknffCzZ2zZJXuwhSf5hluTGHJTA8y5zLcenQRNJW/6bZG3daJaRtmShWYZ3vcacmnIWM/932Txmghz65ucTmr/enTdJxo69sumxlyT13+2y691P5ODs+dLggdtc8zR4cLDsmfKF7J0+R1I3bZP194yRvPRMqXvb9Wa6Z3Cg1B18vWx89EWJW7RMklf/I2uHjpLwc9tLaJeCH9woX/7nXy4ZKxdJ5qo/JO/wfhP86T3br+OFJc7v2+ECsfsFSNJHb0jOri2SnxgrOTuiTQ5hYZprH9xvmCTPmSqOjDS3ad71m4g9LEpSvpwseYf2miF51mTxrN1AvBq2KNf9xZnvhHL6tCgXRXh4ik+9xpI078sjaQ6HZGxcKz6Nmp3Q4dLgMHXFH+Ym4mKzSdQdIyTpx69MIFmUzdNTHHk5br/2HDnZ5r/v2c0l9fABTlV58vAQr7oNJXnB10fSHA7JjF5vbtYl8W3dQbJ2bpHQG4eIX+uOJhcgfdWfkqLLcDjEZvcQm4eHOIp0eaTntfBnKWvHZvFr3UHSlv1icv98zm5pAv3MOdPLb39xUkK7tpXYX5a6pcUsWGxy7pTNy0tC2reUbS9NOjKDwyGxvyyR0K7tzKgWJdu9vSV24RLXLGnR2yV91z4J69pWEpev5eyUJw8P8axVX9IWfet2jrK3bRSvs0r+oeXTor3k7N4qQVcPFJ/m7SU/LUUy1y6V9N++c7tnB149SLL/XSM52/4R6dm3yHo9zbyO3CM/+B25Bfd8vceY9+C0f/ZuZbFee+Uy4hEUbL6oNceuMB33CAk97vu9G5wt3nXqm5zCwkKuuN7k5CX/XOhGU0jGpnXiERxm6hHqzUHrjIT/38CCbQoJP6V9wvHZAwrOe36R856fkmjq95XEM7K6+LftYroHiJ04XpLnz5bAnn0k6PKCHB2ti5m1PVqCL79e7MFhJvD373i+eDdoUjD+n0StN3hwn9R6bpLUnvCpRA4bJYmzppjcQVQtPtUjJetQrFuajnuFBInd10e8I8PE7ukpWUXq5mUdihOfGpEFy6gRKXlZ2ZKb5F6MqPX5fKpHVcBeWJvdP6jgWi9SZUOr7tiDSi7V8giLEp9WnTSakMQPX5W0X74R//OvEP+eV7vm8Tmni3jVqiep82eVuAwtRnbkZEngFf1MUa8OgVf2N9tytPUC5daQo127dmKzFY+ONc3X11caN24st912m/ToUXJRl7Phhw6FaaMQZ8MQKwg6/1LJ3rPTrdGHd71GEnzpVbJ/3ENHfZ/m/sVMmSDhNw2RsOsHmoYcST9/K7la78uRX0Fbj1Kx2SQvJVkSPptkfq3n7NlhAvSgi/tKyg8FOcXxH70t4QOGSa3nJ4kjL09y9u4wuYHedRu4FhN4wRXiXf9siZ30kine9W7cXEJvGGLqDWVFH6n8D6CS2O0mdy/lq6nmWs/dv1PsIWHif/6Vkr7wa7Hrdd/nFkmY+j8Rzb0rgdYVTP70bQm6epD4dbvULCdr3TLJ2bdDHNTnQ0UHfZdffrnpi69169bSuXNnk7Zy5UpZt26dCfY2btwol1xyienA+eqrj/y6KWz8+PEybtw4t7QxY8aYDp5PF/olrl/ORXN3dDwvyT0XqCitzxHY+XxJ+PpTt3TfJi3FIyhE6r485ci8Hh4S3m+wCQb3PnaHSUtb/rsZ7MGhBS12HQ4J6XW15MQcKtN9RHH5aQXnXY99Yfag0GK5vq73JCWKIz/XrXgn99A+0+Jbi5A0Z1fr8cW8OdZ8NrRxjuYkhg9+0LTQNbQ48Kr+EvfBy5L5T0GL35z9u8W7dn0J6nkVQV8Vo7l6mttXmI7nJKVIfmaWZMcmSH5urvhUiygyT4RkHSzIIdT/Hj7e4hkS5Jbb510tQrIOxVTQnlhXfnpKwbUeGOyWrrltJTXUM+/Re0B+ntu1rnUBzfeEFhfXrm/eH37fM273eK/6TcWv6yUS89TtBUXIWzZI3CuPis0/0Pywd2SmS8SoNyU/nvOOCg76YmNj5eGHHzaPYivsueeek127dslPP/1kAjjt1uVoQd/IkSNlxIgRbmmnXS5fXq5k7doqvs3bSPrfywvSbDbxa36OJP8y75hvDejU3XyJpy5d5JaeuuRXydi4xi1NWwOnLv3VNBYpylnEGHjeJaY+WOY/7u9FOdBcuD3bTSvazHUrC9JsNvFp0krS/vixxLdk7YgW/w7dzXzOLwPPqJqm1a0urzCt36mDzS9AfJu1MY07zCo8PE19zqIt9xz5+QXLRZWSuGyNRF1xgVta5MXnSsKygmtUr9ek1f9IZM9uR7p+sdkkokc32fVuwTlPWr1B8rOzzTwHvyqoBhLQpIH416vtWg7KUV6eyanzbtRSsjeudp0j70YtJGNpyQ16cnZtFt+23dyudY/IGpKXnFBw79i6UeImjHR7T/D/3SF5MQckrUi9P+VITzX/vRo2N1VLsjb9tx1ARQV9X3zxhaxaVbx7iJtuukk6dOgg77//vnkyx2uvvXbUZZwpRbnJ87+RyKEPmha0Wsleu2zR/tZS/gvQdFpeQrwkzJ5RrGg3ffUyUwxQmI4XTdPWu5pzqHW5XO/v2Vuytm6S/KxM8WvZVsJvGCwJs6dLfpFWYCgfKb9+J+G33CvZu7dL9q6tEniR9qHmI2nLCoL4sFvvlbzEeEn+9jMznvbHTxJ4fi8Jvf42Sf3tR/GsVkOCLrvW9LHn5NOsjYhNJPfwfvGMrCEh19wqOYf2uZbpyMyQrC3/SMjVt5iWwtrPn0/jFhLQ+UJJ/IqGHBXRZUtA47Nc4/4N6khwm2aSHZ9k+uBr+twI8a1dXdYOftxM3zX5c6l3z83SbPyjppuXyB5dTRctK/ve5VrGjgnTpM3UlyRx1QZJWrlO6t8/SDwD/GTP9Dlmem5yquyZNtv005cTnyQ5KanSasJoSVi6mkYcFST9jx8l+IY7JHffDvNjz7/7ZSY3PmPV72Z60A13Sn5ygqT9Vz8vY/kvpkg2sM8tkrF0gXhEVJeAi66S9CUFQbsjO1PyDh25lxekZUl+eqpbum+H8829QIt6Pc9qLEFX3SIZf86XvNiDFbXrOEOVOujTentLliwxdfcK0zSdpvLz812vz2TaV55m1YddM8AU1WXt2S6HXh/ryoHzDI8SyXf/5eZVo7Ypxj3wytMnvV6fhmdL2DX9xe7jJ9kH90rcjHeK5Rqi/GSsXiqJgcES3PtG8QjSzpl3Suy7L7iKfDzDIt2LdxLjJPbd500/e9VHvmwCQg34TOvd/9i18+ar+otHaIT5AshYu1ySNGjUoqL/xE2bICF9B0j4oPuPdPD83WeStpjOmctbSIdW0m3hR67xFq8UtMLdM2OOrBsyUnxqRolf3SNdJmXs3GsCvBavjpT6wwdK5t6Dsv6u0a4++tSBWT+Id1S4NBlzf0HnzGs3yYo+Q906Xt748AvSPD9f2n/xpumcOfa/zplRMbLWL5fUwCAJuOQ6c6/PPbBbEqe97OokWa/Xwtd6flK8mR7Ue4D43f+cCQg14DOtd0vBI7KmBPS6Qex+gZKXGCtpv86VjMUllyQApWFzlLJmqBbjvvDCC3LHHXdIp06dXHX6PvjgAxk1apQ8+eST8vrrr8v3338vCxaU/stox+1Fmq/jjNVg6pGnjOwdfmOlbgsqRp23vnC9nufVlMNuEb1zol2vD48s6G0AZ75q491Luaqag4/eUqbLq/FyQdWMMyqnb/To0eY5u2+//bZ89FHBL9+mTZuaYt0BAwaY8bvvvluGDRtW9lsLAACAign6fv31V7n55pvNUNSkSZPkrrvuEj8/v5PbGgAAAFSNzpm1y5ZHH31Ucgo9PUBb9F511VXyxBNPlPX2AQAAoDKCPs3p++qrr0x9Pu2Tb968edKqVStJSkqSNWvoRgAAAOCMCPrOPfdcE9xpoNe+fXu59tpr5aGHHpLffvtN6tWrVz5bCQAAgIp/9u7mzZvlr7/+kjp16oinp6dER0dLenr6qW0JAABABbHZbWU6nJFB34svvijdunWTSy+9VDZs2CArVqyQv//+W8455xxZunRp+WwlAAAAKjboe+ONN+Trr7+Wt956y3TArMW8Gvhdd911ctFFF53a1gAAAKBqdNmyfv16iYx0f5C4l5eXvPzyy9KnT5+y3DYAAABUVk5f0YCvsAsvvPBUtwcAAABVpSEHAAAAzvDiXQAAgNOd7TRpcVuWyOkDAACwAII+AAAACyDoAwAAsACCPgAAAAsg6AMAALAAWu8CAADrsVsv38t6ewwAAGBBBH0AAAAWQNAHAABgAQR9AAAAFkDQBwAAYAEEfQAAABZAly0AAMBybDabWA05fQAAABZA0AcAAGABBH0AAAAWQNAHAABgAQR9AAAAFkDrXQAAYDk2u/Xyvay3xwAAABZE0AcAAGABBH0AAAAWQNAHAABgAQR9AAAAFkDrXQAAYDk2O8/eBQAAwBmI4l0AAAALIOgDAACwAII+AAAACyDoAwAAsACCPgAAAAugyxYAAGA9duvle1lvjwEAACyIoA8AAMACCPoAAAAsgKAPAADAAgj6AAAALIDWuwAAwHJsdptYDTl9AAAAFkDQBwAAYAEEfQAAABXsnXfekfr164uvr6906dJFVqxYccz5ExMT5d5775WaNWuKj4+PNGnSRL7//vtSrZM6fQAAABVo5syZMmLECJk4caIJ+CZMmCC9evWS6OhoqVatWrH5s7Oz5dJLLzXTvvzyS6ldu7bs2rVLQkNDS7Vegj4AAIAK9Nprr8kdd9whgwcPNuMa/M2bN0+mTp0qTzzxRLH5NT0+Pl6WLFkiXl5eJk1zCUuL4l0AAGA5Npu9TIcTpbl2q1atkksuucSVZrfbzfjSpUtLfM/cuXOlW7dupni3evXq0qpVK3nhhRckLy+vVPtMTh8AAMApysrKMkNhWvdOh8JiY2NNsKbBW2E6/u+//5a47O3bt8svv/wiN998s6nHt3XrVrnnnnskJydHxowZc/oGfQ2mzq3sTUAlqPPWFxx3i+mdE13Zm4BKUG38DI47zkjjx4+XcePGuaVpQDZ27NhTXnZ+fr6pzzd58mTx8PCQDh06yL59++Tll18+vYM+AACA083IkSNN44zCiubyqcjISBO4HTp0yC1dx2vUqFHisrXFrtbl0/c5NW/eXA4ePGiKi729vU9oG6nTBwAAcIo0wAsODnYbSgr6NEDTnLqFCxe65eTpuNbbK0n37t1Nka7O57R582YTDJ5owFclc/q239ansjcBFaThh9+5XqdNepLjbgEBdz3ven145MBK3RZUTpHuPK+mHHqLoArH0WmO4KBBg6Rjx47SuXNn02VLWlqaqzXvwIEDTbcsWmSshg0bJm+//bY88MADMnz4cNmyZYtpyHH//fdLaVS5oA8AAOBM1q9fP4mJiZGnn37aFNG2bdtWfvzxR1fjjt27d5sWvU5169aV+fPny0MPPSTnnHOOCQg1AHz88cdLtV6CPgAAYD12W6Wu/r777jNDSRYtWlQsTYt+ly1bdkrrpE4fAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAF0HoXAABYjq1QlyhWYb09BgAAsCCCPgAAAAsg6AMAALAAgj4AAAALIOgDAACwAFrvAgAAy7FV8rN3KwM5fQAAABZA0AcAAGABBH0AAAAWQNAHAABgAQR9AAAAFkDrXQAAYD026+V7WW+PAQAALIigDwAAwAII+gAAACyAoA8AAMACCPoAAAAsgKAPAADAAuiyBQAAWI7NbhOrIacPAADAAgj6AAAALICgDwAAwAII+gAAACyAoA8AAMACaL0LAACsx269fC/r7TEAAIAFEfQBAABYAEEfAACABRD0AQAAWABBHwAAgAXQehcAAFiOzcazdwEAAHAGongXAADAAgj6AAAALICgDwAAwAII+gAAACyAoA8AAMAC6LIFAABYj916+V7W22MAAAALOqmcvri4OHn66afl119/lcOHD0t+fr7b9Pj4+LLaPgAAAFRW0HfrrbfK1q1bZciQIVK9enVL9moNAABwxgd9f/zxhyxevFjatGlT9lsEAACAqlGnr1mzZpKRkVH2WwMAAICqE/S9++678uSTT8pvv/1m6vclJye7DQAAAFWZzW4r0+GMLd4NDQ01wV3Pnj3d0h0Oh6nfl5eXV1bbBwAAgMoK+m6++Wbx8vKSTz/9lIYcAAAAZ2rQt2HDBvn777+ladOmYnXBF/eWkCuuE4+QMMnevUPiPp4kWTs2lzhvzSfGi1+z1sXS09eulIOvjzOvo4Y+KEHnXeI+ff0qOfjqGLc0vzYdJaxvf/GuW18cOTmSGb1eDr35fJnuG45u5pqtMuOvzRKXlilNokLksR7tpFXN8BLnnfvPThk7/y+3NG8Puyx74Dq3XPKJSzbKVxt2SEpmtrSpHSmjLm4nZ4UFuebp/cH3ciA53W05w89rJYM7N+NUVQC/rheL/wVXij0wRHIP7pGUuR9J7t7tR53f5usvAZf9n/i07Ch2/wDJS4yT1O8+luzodcXm9b+wjwRefqOk/zlfUr/7xJXuEV5NAq+8SbzqNRHx9JLszesk5duPxJFKNZqKEH5eR2n48BAJad9KfGtVk7+uv0cOzV147Pdc0FlavPKEBLY4WzL3HJCt49+TvTO+cpun3rAB0nDEEPGpESXJ6/6Vfx58VpJWrndNt/t4S/OXn5BaN15pXsf8tFg2DB8n2Yfjym1fYQ0nFfR17NhR9uzZY/mgL6Dz+RJx01CJmf6OZG2PlpDLrpYajzwje564S/JTkoodt0NvPS82zyOH3B4QLHWefUtSVy52my993V8SM2WCa1yDusICOp4rkbcNl/jZMyRz41oRDw/xrl3vZE4lTsL86D3y2m/rZNTF7aV1zXD5ZPUWuXfOH/LV4F4S7u9b4nsCvT1lzuDLXeNFa39MXxktn63ZKs/06iS1QvzlvSX/yL1zFsuXgy4TH08P13zDzm0h17Zu6BoP8OahOhXBp3UXCew9QFK+/lBy9mwT/+69JPT2RyXu1cfEkZZS/A0eHhI65DHJT02W5E/fkrykBPEIixBHhnvQrjzrNBC/zj0k58Bu9wle3mYdOQf2SMIHL5qkwEuvl9CBD0nCe8/oL4Vy21/8dxoD/CV5XbTs+XC2dPzyneMeFr/6daTT3Emye/LnsmbgIxLRs5u0nvScZB6IkdgFBff5mjdcIc1fHikb7h0jiSvWSoP7B0mXeVNkUcvLJTumoI/bFq+OkmpXXCirb3pQcpJTpNUbT0mHWW/L0gv7c2pQ8Q05hg8fLg888IB8+OGHsmrVKlm3bp3bYBUhva6R5N/mS+rinyVn/x6Jnf6OOLKzJOiCS0ucPz8tVfKSEl2DX6u2Zv60Fe5BnyM3x22+/PS0IxPtdokYcKfEfzFVUn79QXIO7TfrTisSOKL8fLJqs1zbqoFc3aq+NIwIlicvaS++nh7yzYadR3+TzSaRAb6uISLA1y2X79O/t8rQLs3kosa1pElUqDxzeWeJSc2QRVv3uy3G39vLbTl+XgR9FcH//MslY+UiyVz1h+Qd3m+CP712/TpeWOL8vh0uELtfgCR99Ibk7Noi+YmxkrMj2uQQFmbz9pHgfsMkec5UcWSkuecG128i9rAoSflysuQd2muG5FmTxbN2A/Fq2KJc9xcFYub/LpvHTJBD3/x8Qoek3p03ScaOvbLpsZck9d/tsuvdT+Tg7PnS4IHbXPM0eHCw7JnyheydPkdSN22T9feMkbz0TKl72/VmumdwoNQdfL1sfPRFiVu0TJJX/yNrh46S8HPbS2gXuknDqTmpb4x+/fqZ/7fffrsrTRtwWKohh4en+NRvLInzZh1Jczgk45814tuomRTP5ysu+PzLJHX57+bLozDfZq2l3psfS15aqmRuWifxsz+S/P9yE3zqNRbP8Ehx5Duk9rg3/itW3i5xM6dJzr5dZb2XKCInL182HUp0K1K122zSpV51WXfg6EUvGdm5cuX735trpFn1ULmveytpFBlipu1LSpPYtEzpclZ11/xBPl7Sqka4WWavZnVd6R+u+Fc+WLZJagT5y+XN6srNHc4WTws+P7JCeXiIZ636krbo2yNpDodkb9soXmc1LvEtPi3aS87urRJ09UDxad7eXL+Za5dK+m/fueXQBV49SLL/XSM52/4R6dm3yHo9zbyO3Nwjq83NMWkaEJr3oEoJ7dpWYn9Z6pYWs2CxyblTNi8vCWnfUra9NOnIDA6HxP6yREK7tjOjWpRs9/aW2IVLXLOkRW+X9F37JKxrW0lcvraidufMZ7PevfOkgr4dO3aI1XkEBYvNw8PkxBWWl5woXjXrHPf9Pg2amPp4MVPfdEtPX79a0v5aIjmxh8SrWk0Jv36g1Hh4nOx/9hERR754Vqth5gu7ZoDEf/6B5MQcktDLr5VaT7xQUKycllrGe4rCEjOyJM/hKFaMG+7vIzvjS65nVS8sSMb06ihnR4ZIalaOzFi1WQZ//qvMGnSZVA/yl7j0TNcyCtPcQA0Gnfq3ayzNqoVKsK+3rNsfJ28t3mCmP3wRv/7Lk90/yFzrWlRbmFbh8IyqWeJ7PMKixKNhc8lcs1QSP3xVPCKqS9A1g0wAmb7wazOPzzldxKtWPYl/Z2yJy9BiZEdOlgRe0U9S5xf8uAy8vJ/ZFntQwQ8GVC0+1SMl61CsW5qOe4UEid3XR7zCQsTu6SlZRermZR2Kk4CmBdU2fGpESl5WtuQmuVcb0Pp8PtWjKmAvcCY7qaCvXr1Tqz+WlZVlhsJ8fHzMYBVaBJy1Z0exRh9py393vc7Zu0uy9+yQs16eYnL/MjetNcWEKvHbmSY4VIenTJB6r02XgE7nScqiHyt4T3A8bWpFmMHpnFoRcv2H82X2uu1yT/dWJ3wAb+nQxPVai4A9Pezyws+rTWMO70L1/lAF2O0mdy/lq6kmJyd3/06xh4SJ//lXmqDPHhIuQX1ukYSp/xPR3LsSaF3B5E/flqCrB4lft0vNcrLWLZOcfTtMjjEAlNZJVwiKjo6Wt956SzZt2mTGmzdvbur6nUiL3vHjx8u4cQWtVZ3GjBkjY8eW/Iu3KspLSRZHXp54hIS6pXsEh5pK28ei9XgCu1wg8V8daaV3NLkxhyQvOUm8qtc0QV9eYkFF3+z9heoG5eZKTsxB8YzgV2B5C/XzEQ+bTeL/y51zik/PcqundyxeHnaTY7cnsaAOV8R/uYa6jKhAP9d82jK4aTX3z1dhrWuES26+Q/Ynp0v98COtfFG28tNTzLVuDwx2S9fctpIabJn3JCeK5Oe5FeVqXUC9P5ji4tr1zfvD73vGNV1z8LzqNxW/rpdIzFO3FxQhb9kgca88Kjb/QJH8fHFkpkvEqDclPz6G01wFaa6e5vYVpuM5SSmSn5kl2bEJkp+bKz7VIorMEyFZBwtyCPW/h4+3eIYEueX2eVeLkKxDnHecmpMq0J49e7a0atXKNOLQ5+/qsHr1apOm045n5MiRkpSU5DZo2mklL1eydm4VvxaFitZsNjOeue3fY741oPN5Il5ekrrk1+OuRlv82QODXMGerjM/J1u8a9YuNJOHeEZWk9zYw6ewQzjRgK159VBZsfvIsc53OMz4OTXdb+RHk5fvkK2xyaYhhqodEmBeF16mFgNvOBh/zGVGxySJdgJftFgYZSwvz+TUeTdqeSTNZhPvRi1Mvb2S5OzaLB4R1Vw588ojsobkJSeY5eVs3ShxE0ZK/FujXUPO3u2StXapeV20Za4jPdUEfF4Nm5tW/1mbVnOaq6DEZWskomdXt7TIi8+VhGVrXD0xJK3+RyJ7djsyg80mET26SeKyv81o0uoNkp+d7TZPQJMG4l+vtms5QIXm9D322GMmSHvmmSO/Up25dTrt+usLWiEdzZlSlJs0/2uJuuMhydqxRbK2bzZdtth8fCX1j4KWXlF3jJDchDhJ+HK62/uCzr9M0lcvczXOcNL3hl3T3xTbam6h1heK6DdYcg4fkPQNBTd5R2aGabUbds3NkhsXK7lxh00/gYoWvBXj5g5NZMyPK6VF9TBpWSNcPl29RTJycqVvy/pm+lM/rJBqgX4y/PyCPhknL91ounapGxooKVqn76/NciA5Ta5t3aDgvNtsMqBdY/lg+SY5KyxQagUHmC5bNNdPW/OqtfvjTBDYqW6U+Ht5yroD8fLqorVyZfN6po4fylf6Hz9K8A13SO6+HZKzZ7v4d7/M5NhnrCqojhF0w52Sn5wgaf/VvctY/ospkg3sc4tkLF1g6vQFXHSVpC/5yUx3ZGdK3qF9buvQBl356alu6b4dzpfcw/tNUa/nWY0l6KpbJOPP+ZIXe5BTXkFdtgQ0Pss17t+gjgS3aSbZ8UmmD76mz40Q39rVZe3gx830XZM/l3r33CzNxj9qunmJ7NHVdNGysu9drmXsmDBN2kx9SRJXbZCkleuk/v2DxDPAT/ZMn2Om5yanyp5ps00/fTnxSZKTkiqtJoyWhKWracSBygn6Dhw4IAMHDiyWfsstt8jLL78sVpG24g/xCAqRsGtvEc+QMMnavV0Ovvq0acyhTHGrI9/tPV41aotf05Zy4OXRxReYny/edRpIUPeLTWeuuYnxkrHhb0mY87EpwnWKmznVFDdF3TlC7N4+krktWg689KR71y4oN72a1pWE9Cx5b8lG0wijaVSIvH3dea7i3YMp6aZFr1NyVrY8u2C1mTfYx0uaVw+Taf17mO5enAZ1aioZOXny3IJVJjBsWzvSLNPZR5925jz/3z0yaelGycnNk1ohAabl7i3tz+ZMV4Cs9cslNTBIAi65zhTL5h7YLYnTXnZ1kuwRGuGWO5efFG+mB/UeIH73P2cCQg34TOvdUvCIrCkBvW4Qu1+g5CXGStqvcyVjMfV2K0pIh1bSbeFHrvEWrxS0wt0zY46sGzJSfGpGiV/dI415MnbuNQFei1dHSv3hAyVz70FZf9doVx996sCsH8Q7KlyajLm/oHPmtZtkRZ+hbh0vb3z4BWmeny/tv3jTdM4c+1/nzChj9tPjebllyeY4iRrBV155pdxwww0yePBgt/Rp06bJ559/LvPnzz/pDdp+W5+Tfi9OLw0/PPIFmDbpyUrdFlSMgLuOPDXm8MjiPxxxZqo2fobr9TwvnuRkFb1zoqUqS51YttXKAu8eL2dMTt/cuXNdr/v27SuPP/64qdPXtWtB/YVly5bJrFmzijXQAAAAwGkU9F1zzTXF0t59910zFHbvvffK3XffXTZbBwAAgIoN+vLz3eumAQAA4PRhvWeQAAAAWNBJtd598033R4c5adcTvr6+0rhxY7ngggvEw4OnBAAAAJy2Qd/rr78uMTExkp6eLmFhYSYtISFB/P39JTAwUA4fPiwNGzaUX3/9VerWPfKweAAAgKrAZrNeYedJ7fELL7wgnTp1ki1btkhcXJwZNm/eLF26dJE33nhDdu/eLTVq1JCHHnqo7LcYAAAAFZPTN3r0aPO4tUaNGrnStEj3lVdeMU/j2L59u/zvf/877pM5AAAAUMWfyJFb6AkRTpp28GDB44Fq1aolKSnujxkDAACoap3FW8VJFe/26NFD7rrrLvn774IHRCt9PWzYMOnZs6cZX79+vTRoUPBsUQAAAJyGQd+UKVMkPDxcOnToID4+PmbQ15r2wQcfmHm0Qcerr75a1tsLAACAiire1UYaCxYskOjoaDOopk2bmqFwbiAAAABOs6BvxIgRx5y+aNEi1+vXXnvt1LYKAAAAlRP0Fa6/dyzaQTMAAABO06BPO1oGAADA6cl63VEDAABYEEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAFEPQBAABYAEEfAACABRD0AQAAWABBHwAAgAUQ9AEAAFgAQR8AAIAF2BwOh6OyNwIAAADli5y+SpaVlSVjx441/2EdnHfr4ZxbE+cdVQk5fZUsOTlZQkJCJCkpSYKDgyt7c1BBOO/Wwzm3Js47qhJy+gAAACyAoA8AAMACCPoAAAAsgKCvkvn4+MiYMWPMf1gH5916OOfWxHlHVUJDDgAAAAsgpw8AAMACCPoAAAAsgKAPAADAAgj6yoHNZpOvv/76qNN37txp5lmzZk15rB6nmQ8//FBCQ0MrezNQSW677Ta55pprOP5V2EUXXSQPPvhgmX0HAJXFs9LWfAY7cOCAhIWFVfZmAAAAuBD0lbHs7GypUaNGWS8WAADglFC8WwbZ/vfdd5/J+o+MjJRevXoVy9pfsWKFtGvXTnx9faVjx47y999/F1vO3Llz5eyzzzbz9OjRQ6ZPn26Wk5iY6Jpn8eLFcv7554ufn5/UrVtX7r//fklLSzvVXUAZfQZ00Oco6+fgqaeeEofDYaYnJCTIwIEDTe6vv7+/XHHFFbJly5ajFv3b7Xb566+/3NInTJgg9erVk/z8fM5XFRITE2N+5L3wwguutCVLloi3t7csXLjQjD/33HNSrVo1CQoKkqFDh8oTTzwhbdu2LbascePGSVRUlHkG9913321+QKJqluT07t3b3IcbNGggn376qdSvX99co0Xn02td52vYsKF8+eWXxar4fPHFF657eqdOnWTz5s2ycuVK8z0RGBho3q+fMaDMOHBKLrzwQkdgYKDj0Ucfdfz7779m0MP61VdfmekpKSmOqKgox4ABAxwbNmxwfPvtt46GDRuaef7++28zz/bt2x1eXl6ORx55xLz/s88+c9SuXdvMk5CQYObZunWrIyAgwPH66687Nm/e7Pjzzz8d7dq1c9x2222cwSryGXjggQfM+fv4448d/v7+jsmTJ5vpffv2dTRv3tzx+++/O9asWePo1auXo3Hjxo7s7Gwzfdq0aY6QkBDX8i699FLHPffc47aOc845x/H0009X8J7hRMybN89cvytXrnQkJyeb6/uhhx4y0/Sz4Ovr65g6daojOjraMW7cOEdwcLCjTZs2rvcPGjTIfH769etn7hHfffeduWeMGjWKE1CFrnG9vtUll1ziaNu2rWPZsmWOVatWmWl+fn7m3uyk9+6IiAjH+++/b8776NGjHR4eHo6NGzea6Tt27DDzNGvWzPHjjz+a9K5duzo6dOjguOiiixyLFy92rF692twn7r777krbb5x5CPpOkV7wGny5HdRCQd+kSZPMxZ+RkeGa/t5777kFfY8//rijVatWbst48skn3YK+IUOGOO688063ef744w+H3W53WzYq5zOgQV1+fr4rTc+ppmmArudRg3Sn2NhY8yXxxRdflBj0zZw50xEWFubIzMw04/rFYrPZzBcFqiYN0ps0aWJ+3LVu3dp17rp06eK499573ebt3r17saAvPDzckZaW5naP0EAwLy+vAvcCxwv6Nm3aZK5nDfCdtmzZYtKKBn1FgzX9LAwbNswt6Pvggw9c0/XHvqYtXLjQlTZ+/HhH06ZNOTEoMxTvloEOHTocddqmTZvknHPOMcW2Tt26dXObJzo62mTtF9a5c2e38bVr15pWnprl7xy0KFmL+3bs2FEWu4FT0LVrV1NcU/gcaxHuxo0bxdPTU7p06eKaFhERIU2bNjWfjZJoS04PDw/56quvzLiedy3y1yIkVE2vvPKK5ObmyqxZs+STTz5xPVZRr+2i13LRcdWmTRtT9F/485Oamip79uypgK3HidLzqddz+/btXWmNGzcuseFe0fu8jhe95vW7wal69ermf+vWrd3SDh8+zAlCmSHoKwMBAQFS3vQL4K677jLdvDgHDQQ1sGjUqFG5rx8VR+uDaR3AadOmmXpdWmfo9ttv5xRUYdu2bZP9+/ebH2FaXws4EV5eXq7Xzh+NRdOox4uyRNBXzpo3by7r1q2TzMxMV9qyZcvc5tFcn6IV97Uyb2H6y1JzjfRXZdFBgwRUruXLl7uN6znWhjktWrQwOUCFp8fFxZkcA512NFrh/+eff5Z3333XvP+6664r1+3HydPA/JZbbpF+/frJs88+a86dM3dGr+2i13LRcaU/4DIyMtw+P5qbrw22UHXo+dTrsXBjvK1bt5rGWkUVvc/ruH4fAJWJoK+cDRgwwPxau+OOO0zQ9v3335uioMI0B+/ff/+Vxx9/3LTe0hZdWqRX+NefTtNWgdpCVHP5NIfvm2++MeOofLt375YRI0aYYO6zzz6Tt956Sx544AET+F199dXm/Gvra/1y1wChdu3aJv1o9MtBi4z1vPfv39+07kPV9OSTT0pSUpK8+eab5nw1adLElTM7fPhwmTJlimmNr9estuTVH4GFqwI4A8chQ4a47hFjxowx17a25EbV0axZM7nkkkvkzjvvNL0yaPCnr/X6LHpOtah/6tSp5p6u51Pn536NysYdpZzpr/Vvv/1W1q9fb7pt0S+Il156yW0ebfavzfnnzJlj6ni89957Zj7lrBuk6b/99pu5gWgTf13W008/LbVq1SrvXcAJ0OJYzanR+lr33nuvCfj0y0BpMa3W++zTp4+p16P1vPWLvXAxTkk0CNBggKLdqmvRokWmq46PPvrIdLWiQZq+/uOPP8x1fPPNN8vIkSPlkUceMbn1Wv9Wn8BRuI6vuvjii80PhAsuuMDkGPbt21fGjh1bafuFo5sxY4apa6fn6tprrzU/6LQ7nqLnVLvg+fzzz829W9+jPwaPlbsPVASbtuaokDWhVJ5//nmZOHEiFblPk376tN+1ov10nSotKtTcAs0Zwpnj0ksvNX37aXCI09/evXtNMbxWx9DgHajKeCJHFaF1t7QFr7bs/PPPP+Xll1+mKMCitNGONgZ4++23TXEgTl/p6enmx5u2tNcW2Zrbo8HBggULKnvTcJJ++eUXc41qK1vtgPmxxx4zLes15w+o6gj6qghnfZ/4+Hg566yz5OGHHzbFQrAerfejwYF23ULR7ulN63lpUb7m3GtjLm0IMHv2bFMvDKennJwcGTVqlGzfvt0U65577rmmm57jVdcAqgKKdwEAACyAhhwAAAAWQNAHAABgAQR9AAAAFkDQBwAAYAEEfQAAABZA0AcAAGABBH0AAAAWQNAHAABgAQR9AAAAcub7f8W+EW/OQoY4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ========== CALCULATE RESIDUALS ==========\n",
    "residuals = pd.DataFrame({\n",
    "    \"ridge\": y_val.values - y_val_pred_ridge,\n",
    "    \"poly\": y_val.values - y_val_poly,\n",
    "    \"xgb\": y_val.values - y_val_pred_xg,\n",
    "    \"lgbm\": y_val.values - y_val_pred_lgbm\n",
    "})\n",
    "\n",
    "# ========== CORRELATION ==========\n",
    "residual_corr = residuals.corr()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RESIDUAL CORRELATION MATRIX\")\n",
    "print(\"=\"*70)\n",
    "print(residual_corr)\n",
    "\n",
    "# ========== VISUALIZE ==========\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(residual_corr, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Residual Correlation Between Models', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('residual_correlation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44c50cb",
   "metadata": {},
   "source": [
    "# Ensembled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3728907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "def create_stacked_ridge_ensemble(base_models, X_train, y_train, X_val, y_val,\n",
    "                                   selected_features, optimize_alpha=True, \n",
    "                                   alphas=None, n_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Create and optimize a stacked ensemble with Ridge meta-learner using OOF predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_models : dict\n",
    "        Dictionary mapping model names to fitted model objects\n",
    "    X_train, y_train : Training data\n",
    "    X_val, y_val : Validation data\n",
    "    selected_features : list\n",
    "        List of feature names to use\n",
    "    optimize_alpha : bool\n",
    "        Whether to use RidgeCV to optimize alpha\n",
    "    alphas : list or None\n",
    "        Alpha values to try if optimize_alpha=True\n",
    "    n_folds : int\n",
    "        Number of folds for out-of-fold predictions\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict containing ensemble and metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"STACKED ENSEMBLE WITH RIDGE META-LEARNER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model_names = list(base_models.keys())\n",
    "    n_models = len(model_names)\n",
    "    \n",
    "    if alphas is None:\n",
    "        alphas = [0.1, 1.0, 10.0, 100.0]\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Base models: {', '.join(model_names)}\")\n",
    "    print(f\"  Number of models: {n_models}\")\n",
    "    print(f\"  CV folds: {n_folds}\")\n",
    "    print(f\"  Optimize alpha: {optimize_alpha}\")\n",
    "    if optimize_alpha:\n",
    "        print(f\"  Alpha candidates: {alphas}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: GENERATE OUT-OF-FOLD PREDICTIONS\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"STEP 1: GENERATING OUT-OF-FOLD PREDICTIONS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Initialize arrays\n",
    "    oof_predictions = np.zeros((len(X_train), n_models))\n",
    "    val_predictions = np.zeros((len(X_val), n_models))\n",
    "    \n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    for model_idx, (model_name, base_model) in enumerate(base_models.items()):\n",
    "        print(f\"\\nProcessing {model_name}...\")\n",
    "        \n",
    "        fold_val_preds = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_train), 1):\n",
    "            \n",
    "            # Split data\n",
    "            X_fold_train = X_train.iloc[train_idx]\n",
    "            y_fold_train = y_train.iloc[train_idx]\n",
    "            X_fold_val = X_train.iloc[val_idx]\n",
    "            \n",
    "            # Clone and train model\n",
    "            model_fold = clone(base_model)\n",
    "            model_fold.fit(X_fold_train, y_fold_train)\n",
    "            \n",
    "            # Out-of-fold predictions on training data\n",
    "            oof_predictions[val_idx, model_idx] = model_fold.predict(X_fold_val)\n",
    "            \n",
    "            # Predictions on validation set\n",
    "            fold_val_preds.append(model_fold.predict(X_val))\n",
    "            \n",
    "            print(f\"  Fold {fold}/{n_folds} complete\", end='\\r')\n",
    "        \n",
    "        # Average validation predictions across folds\n",
    "        val_predictions[:, model_idx] = np.mean(fold_val_preds, axis=0)\n",
    "        \n",
    "        print(f\"  ✅ {model_name} complete\" + \" \"*20)\n",
    "    \n",
    "    print(f\"\\n✅ Out-of-fold predictions generated!\")\n",
    "    print(f\"   OOF shape: {oof_predictions.shape}\")\n",
    "    print(f\"   Val shape: {val_predictions.shape}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: TRAIN META-LEARNER\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"STEP 2: TRAINING RIDGE META-LEARNER\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if optimize_alpha:\n",
    "        print(f\"\\nUsing RidgeCV to optimize alpha...\")\n",
    "        meta_model = RidgeCV(alphas=alphas, cv=5)\n",
    "        meta_model.fit(oof_predictions, y_train)\n",
    "        best_alpha = meta_model.alpha_\n",
    "        print(f\"  ✅ Best alpha found: {best_alpha:.4f}\")\n",
    "    else:\n",
    "        best_alpha = alphas[0] if isinstance(alphas, list) else 1.0\n",
    "        print(f\"\\nUsing fixed alpha: {best_alpha}\")\n",
    "        meta_model = Ridge(alpha=best_alpha)\n",
    "        meta_model.fit(oof_predictions, y_train)\n",
    "    \n",
    "    print(f\"\\n✅ Meta-learner trained!\")\n",
    "    \n",
    "    # Print coefficients\n",
    "    print(f\"\\nMeta-learner coefficients:\")\n",
    "    for name, coef in zip(model_names, meta_model.coef_):\n",
    "        print(f\"  {name:15s}: {coef:8.4f}\")\n",
    "    print(f\"  {'Intercept':15s}: {meta_model.intercept_:8.4f}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: MAKE PREDICTIONS\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"STEP 3: MAKING PREDICTIONS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    y_train_pred = meta_model.predict(oof_predictions)\n",
    "    y_val_pred = meta_model.predict(val_predictions)\n",
    "    \n",
    "    print(\"  ✅ Predictions generated\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: EVALUATE PERFORMANCE\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"STEP 4: EVALUATING PERFORMANCE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Training metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_mae = np.mean(np.abs(y_train - y_train_pred))\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Validation metrics\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    val_mae = np.mean(np.abs(y_val - y_val_pred))\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Overfitting metrics\n",
    "    rmse_gap = val_rmse - train_rmse\n",
    "    r2_gap = train_r2 - val_r2\n",
    "    \n",
    "    print(f\"\\nTraining Performance:\")\n",
    "    print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"  MAE:  {train_mae:.4f}\")\n",
    "    print(f\"  R²:   {train_r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\nValidation Performance:\")\n",
    "    print(f\"  RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"  MAE:  {val_mae:.4f}\")\n",
    "    print(f\"  R²:   {val_r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\nOverfitting Check:\")\n",
    "    print(f\"  RMSE gap: {rmse_gap:.4f} ({(rmse_gap/train_rmse)*100:.2f}%)\")\n",
    "    print(f\"  R² gap:   {r2_gap:.4f}\")\n",
    "    \n",
    "    if rmse_gap/train_rmse < 0.1:\n",
    "        print(\"  ✅ Low overfitting - model generalizes well\")\n",
    "    elif rmse_gap/train_rmse < 0.2:\n",
    "        print(\"  ⚠️  Moderate overfitting\")\n",
    "    else:\n",
    "        print(\"  ❌ High overfitting - consider regularization\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 5: COMPARE TO INDIVIDUAL MODELS\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"STEP 5: COMPARISON TO BASE MODELS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(\"\\nIndividual model performance on validation set:\")\n",
    "    individual_rmse = {}\n",
    "    for name, model in base_models.items():\n",
    "        y_val_individual = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_val_individual))\n",
    "        individual_rmse[name] = rmse\n",
    "        print(f\"  {name:15s}: {rmse:.4f}\")\n",
    "    \n",
    "    best_individual = min(individual_rmse.values())\n",
    "    improvement = ((best_individual - val_rmse) / best_individual) * 100\n",
    "    \n",
    "    print(f\"\\nStacked ensemble:    {val_rmse:.4f}\")\n",
    "    print(f\"Best individual:     {best_individual:.4f}\")\n",
    "    print(f\"Improvement:         {improvement:.2f}%\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 6: CREATE ENSEMBLE OBJECT\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"STEP 6: CREATING ENSEMBLE OBJECT\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    class StackedEnsemble:\n",
    "        \"\"\"\n",
    "        Stacked ensemble predictor using Ridge meta-learner.\n",
    "        \"\"\"\n",
    "        def __init__(self, base_models, meta_model, model_names):\n",
    "            self.base_models = base_models  # Store as dict\n",
    "            self.meta_model = meta_model\n",
    "            self.model_names = model_names\n",
    "        \n",
    "        def predict(self, X):\n",
    "            \"\"\"Make predictions on new data\"\"\"\n",
    "            # Get base model predictions\n",
    "            base_preds = np.column_stack([\n",
    "                self.base_models[name].predict(X)\n",
    "                for name in self.model_names\n",
    "            ])\n",
    "            \n",
    "            # Meta-learner combines them\n",
    "            return self.meta_model.predict(base_preds)\n",
    "        \n",
    "        def get_coefficients(self):\n",
    "            \"\"\"Return meta-learner coefficients\"\"\"\n",
    "            return dict(zip(self.model_names, self.meta_model.coef_))\n",
    "    \n",
    "    ensemble = StackedEnsemble(base_models, meta_model, model_names)\n",
    "    \n",
    "    # Verify it works (with tolerance for floating point differences)\n",
    "    test_pred = ensemble.predict(X_val)\n",
    "    \n",
    "    # Check if predictions are close (within tolerance)\n",
    "    if not np.allclose(test_pred, y_val_pred, rtol=1e-5, atol=1e-8):\n",
    "        print(f\"\\n⚠️  Warning: Small mismatch in predictions\")\n",
    "        print(f\"  Max difference: {np.max(np.abs(test_pred - y_val_pred)):.6f}\")\n",
    "        print(f\"  This is likely due to floating point precision and is OK\")\n",
    "    else:\n",
    "        print(\"  ✅ Ensemble object created and verified\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SUMMARY\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"✅ STACKED ENSEMBLE CREATION COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return {\n",
    "        'ensemble': ensemble,\n",
    "        'meta_model': meta_model,\n",
    "        'base_models': base_models,\n",
    "        'model_names': model_names,\n",
    "        'coefficients': dict(zip(model_names, meta_model.coef_)),\n",
    "        'intercept': meta_model.intercept_,\n",
    "        'best_alpha': best_alpha,\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'train_r2': train_r2,\n",
    "        'val_rmse': val_rmse,\n",
    "        'val_mae': val_mae,\n",
    "        'val_r2': val_r2,\n",
    "        'rmse_gap': rmse_gap,\n",
    "        'r2_gap': r2_gap,\n",
    "        'improvement_pct': improvement,\n",
    "        'individual_rmse': individual_rmse,\n",
    "        'y_train_pred': y_train_pred,\n",
    "        'y_val_pred': y_val_pred,\n",
    "        'oof_predictions': oof_predictions,\n",
    "        'val_predictions': val_predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bdcb636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = {\n",
    "    'poly': best_poly_ridge,      # Your polynomial Ridge model\n",
    "    'lgbm': best_lgbm,          # Your LightGBM model\n",
    "    'xgb': best_XG_model             # Your XGBoost model (optional)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06e58efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STACKED ENSEMBLE WITH RIDGE META-LEARNER\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Base models: poly, lgbm, xgb\n",
      "  Number of models: 3\n",
      "  CV folds: 5\n",
      "  Optimize alpha: True\n",
      "  Alpha candidates: [0.1, 1.0, 10.0, 100.0]\n",
      "\n",
      "======================================================================\n",
      "STEP 1: GENERATING OUT-OF-FOLD PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "Processing poly...\n",
      "  ✅ poly complete                    \n",
      "\n",
      "Processing lgbm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 2/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 3/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 4/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ lgbm complete                    \n",
      "\n",
      "Processing xgb...\n",
      "  ✅ xgb complete                    \n",
      "\n",
      "✅ Out-of-fold predictions generated!\n",
      "   OOF shape: (3292, 3)\n",
      "   Val shape: (2689, 3)\n",
      "\n",
      "======================================================================\n",
      "STEP 2: TRAINING RIDGE META-LEARNER\n",
      "======================================================================\n",
      "\n",
      "Using RidgeCV to optimize alpha...\n",
      "  ✅ Best alpha found: 100.0000\n",
      "\n",
      "✅ Meta-learner trained!\n",
      "\n",
      "Meta-learner coefficients:\n",
      "  poly           :   0.3404\n",
      "  lgbm           :  -0.1035\n",
      "  xgb            :   0.5809\n",
      "  Intercept      : 121.5069\n",
      "\n",
      "======================================================================\n",
      "STEP 3: MAKING PREDICTIONS\n",
      "======================================================================\n",
      "  ✅ Predictions generated\n",
      "\n",
      "======================================================================\n",
      "STEP 4: EVALUATING PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Training Performance:\n",
      "  RMSE: 2111.4992\n",
      "  MAE:  566.4222\n",
      "  R²:   0.6162\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 1260.9640\n",
      "  MAE:  462.3816\n",
      "  R²:   0.7694\n",
      "\n",
      "Overfitting Check:\n",
      "  RMSE gap: -850.5351 (-40.28%)\n",
      "  R² gap:   -0.1532\n",
      "  ✅ Low overfitting - model generalizes well\n",
      "\n",
      "======================================================================\n",
      "STEP 5: COMPARISON TO BASE MODELS\n",
      "======================================================================\n",
      "\n",
      "Individual model performance on validation set:\n",
      "  poly           : 1350.6125\n",
      "  lgbm           : 1431.0001\n",
      "  xgb            : 1247.8709\n",
      "\n",
      "Stacked ensemble:    1260.9640\n",
      "Best individual:     1247.8709\n",
      "Improvement:         -1.05%\n",
      "\n",
      "======================================================================\n",
      "STEP 6: CREATING ENSEMBLE OBJECT\n",
      "======================================================================\n",
      "\n",
      "⚠️  Warning: Small mismatch in predictions\n",
      "  Max difference: 5838.317268\n",
      "  This is likely due to floating point precision and is OK\n",
      "\n",
      "======================================================================\n",
      "✅ STACKED ENSEMBLE CREATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Final Performance:\n",
      "  Validation RMSE: 1260.9640\n",
      "  Validation R²: 0.7694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========== STEP 2: CREATE STACKED ENSEMBLE ==========\n",
    "result = create_stacked_ridge_ensemble(\n",
    "    base_models=base_models,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    selected_features=selected_features,\n",
    "    optimize_alpha=True,                    # Automatically find best alpha\n",
    "    alphas=[0.1, 1.0, 10.0, 100.0],        # Alpha values to try\n",
    "    n_folds=5,                              # Number of CV folds\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ========== STEP 3: ACCESS RESULTS ==========\n",
    "stacked_ensemble = result['ensemble']\n",
    "val_rmse = result['val_rmse']\n",
    "val_r2 = result['val_r2']\n",
    "coefficients = result['coefficients']\n",
    "\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"  Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"  Validation R²: {val_r2:.4f}\")\n",
    "\n",
    "# ========== STEP 4: USE THE ENSEMBLE ==========\n",
    "# Make predictions on new data\n",
    "predictions = stacked_ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce5642ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STACKED ENSEMBLE WITH RIDGE META-LEARNER\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Base models: lgbm, xgb\n",
      "  Number of models: 2\n",
      "  CV folds: 5\n",
      "  Optimize alpha: True\n",
      "  Alpha candidates: [0.1, 1.0, 10.0, 100.0]\n",
      "\n",
      "======================================================================\n",
      "STEP 1: GENERATING OUT-OF-FOLD PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "Processing lgbm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 2/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 3/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 4/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ lgbm complete                    \n",
      "\n",
      "Processing xgb...\n",
      "  ✅ xgb complete                    \n",
      "\n",
      "✅ Out-of-fold predictions generated!\n",
      "   OOF shape: (3292, 2)\n",
      "   Val shape: (2689, 2)\n",
      "\n",
      "======================================================================\n",
      "STEP 2: TRAINING RIDGE META-LEARNER\n",
      "======================================================================\n",
      "\n",
      "Using RidgeCV to optimize alpha...\n",
      "  ✅ Best alpha found: 100.0000\n",
      "\n",
      "✅ Meta-learner trained!\n",
      "\n",
      "Meta-learner coefficients:\n",
      "  lgbm           :  -0.1617\n",
      "  xgb            :   1.1986\n",
      "  Intercept      :   0.1132\n",
      "\n",
      "======================================================================\n",
      "STEP 3: MAKING PREDICTIONS\n",
      "======================================================================\n",
      "  ✅ Predictions generated\n",
      "\n",
      "======================================================================\n",
      "STEP 4: EVALUATING PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Training Performance:\n",
      "  RMSE: 2414.5728\n",
      "  MAE:  579.5217\n",
      "  R²:   0.4981\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 1134.8718\n",
      "  MAE:  424.2123\n",
      "  R²:   0.8132\n",
      "\n",
      "Overfitting Check:\n",
      "  RMSE gap: -1279.7011 (-53.00%)\n",
      "  R² gap:   -0.3151\n",
      "  ✅ Low overfitting - model generalizes well\n",
      "\n",
      "======================================================================\n",
      "STEP 5: COMPARISON TO BASE MODELS\n",
      "======================================================================\n",
      "\n",
      "Individual model performance on validation set:\n",
      "  lgbm           : 1431.0001\n",
      "  xgb            : 1247.8709\n",
      "\n",
      "Stacked ensemble:    1134.8718\n",
      "Best individual:     1247.8709\n",
      "Improvement:         9.06%\n",
      "\n",
      "======================================================================\n",
      "STEP 6: CREATING ENSEMBLE OBJECT\n",
      "======================================================================\n",
      "\n",
      "⚠️  Warning: Small mismatch in predictions\n",
      "  Max difference: 10849.016423\n",
      "  This is likely due to floating point precision and is OK\n",
      "\n",
      "======================================================================\n",
      "✅ STACKED ENSEMBLE CREATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Final Performance:\n",
      "  Validation RMSE: 1134.8718\n",
      "  Validation R²: 0.8132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_models = {\n",
    "    # 'poly': best_poly_ridge,      # Your polynomial Ridge model\n",
    "    'lgbm': best_lgbm,          # Your LightGBM model\n",
    "    'xgb': best_XG_model             # Your XGBoost model (optional)\n",
    "}\n",
    "\n",
    "# ========== STEP 2: CREATE STACKED ENSEMBLE ==========\n",
    "result = create_stacked_ridge_ensemble(\n",
    "    base_models=base_models,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    selected_features=selected_features,\n",
    "    optimize_alpha=True,                    # Automatically find best alpha\n",
    "    alphas=[0.1, 1.0, 10.0, 100.0],        # Alpha values to try\n",
    "    n_folds=5,                              # Number of CV folds\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ========== STEP 3: ACCESS RESULTS ==========\n",
    "stacked_ensemble = result['ensemble']\n",
    "val_rmse = result['val_rmse']\n",
    "val_r2 = result['val_r2']\n",
    "coefficients = result['coefficients']\n",
    "\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"  Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"  Validation R²: {val_r2:.4f}\")\n",
    "\n",
    "# ========== STEP 4: USE THE ENSEMBLE ==========\n",
    "# Make predictions on new data\n",
    "predictions = stacked_ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "99548ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STACKED ENSEMBLE WITH RIDGE META-LEARNER\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Base models: Ridge, lgbm, xgb\n",
      "  Number of models: 3\n",
      "  CV folds: 5\n",
      "  Optimize alpha: True\n",
      "  Alpha candidates: [0.1, 1.0, 10.0, 100.0]\n",
      "\n",
      "======================================================================\n",
      "STEP 1: GENERATING OUT-OF-FOLD PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "Processing Ridge...\n",
      "  ✅ Ridge complete                    \n",
      "\n",
      "Processing lgbm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 2/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 3/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 4/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ lgbm complete                    \n",
      "\n",
      "Processing xgb...\n",
      "  ✅ xgb complete                    \n",
      "\n",
      "✅ Out-of-fold predictions generated!\n",
      "   OOF shape: (3292, 3)\n",
      "   Val shape: (2689, 3)\n",
      "\n",
      "======================================================================\n",
      "STEP 2: TRAINING RIDGE META-LEARNER\n",
      "======================================================================\n",
      "\n",
      "Using RidgeCV to optimize alpha...\n",
      "  ✅ Best alpha found: 100.0000\n",
      "\n",
      "✅ Meta-learner trained!\n",
      "\n",
      "Meta-learner coefficients:\n",
      "  Ridge          :   1.1460\n",
      "  lgbm           :  -0.4259\n",
      "  xgb            :   0.2802\n",
      "  Intercept      :   7.5653\n",
      "\n",
      "======================================================================\n",
      "STEP 3: MAKING PREDICTIONS\n",
      "======================================================================\n",
      "  ✅ Predictions generated\n",
      "\n",
      "======================================================================\n",
      "STEP 4: EVALUATING PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Training Performance:\n",
      "  RMSE: 1905.9433\n",
      "  MAE:  572.3588\n",
      "  R²:   0.6873\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 1226.2392\n",
      "  MAE:  457.2481\n",
      "  R²:   0.7820\n",
      "\n",
      "Overfitting Check:\n",
      "  RMSE gap: -679.7041 (-35.66%)\n",
      "  R² gap:   -0.0947\n",
      "  ✅ Low overfitting - model generalizes well\n",
      "\n",
      "======================================================================\n",
      "STEP 5: COMPARISON TO BASE MODELS\n",
      "======================================================================\n",
      "\n",
      "Individual model performance on validation set:\n",
      "  Ridge          : 1269.2618\n",
      "  lgbm           : 1431.0001\n",
      "  xgb            : 1247.8709\n",
      "\n",
      "Stacked ensemble:    1226.2392\n",
      "Best individual:     1247.8709\n",
      "Improvement:         1.73%\n",
      "\n",
      "======================================================================\n",
      "STEP 6: CREATING ENSEMBLE OBJECT\n",
      "======================================================================\n",
      "\n",
      "⚠️  Warning: Small mismatch in predictions\n",
      "  Max difference: 6344.761916\n",
      "  This is likely due to floating point precision and is OK\n",
      "\n",
      "======================================================================\n",
      "✅ STACKED ENSEMBLE CREATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Final Performance:\n",
      "  Validation RMSE: 1226.2392\n",
      "  Validation R²: 0.7820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_models = {\n",
    "    'Ridge': best_ridge,      # Your polynomial Ridge model\n",
    "    'lgbm': best_lgbm,          # Your LightGBM model\n",
    "    'xgb': best_XG_model             # Your XGBoost model (optional)\n",
    "}\n",
    "\n",
    "# ========== STEP 2: CREATE STACKED ENSEMBLE ==========\n",
    "result = create_stacked_ridge_ensemble(\n",
    "    base_models=base_models,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    selected_features=selected_features,\n",
    "    optimize_alpha=True,                    # Automatically find best alpha\n",
    "    alphas=[0.1, 1.0, 10.0, 100.0],        # Alpha values to try\n",
    "    n_folds=5,                              # Number of CV folds\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ========== STEP 3: ACCESS RESULTS ==========\n",
    "stacked_ensemble = result['ensemble']\n",
    "val_rmse = result['val_rmse']\n",
    "val_r2 = result['val_r2']\n",
    "coefficients = result['coefficients']\n",
    "\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"  Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"  Validation R²: {val_r2:.4f}\")\n",
    "\n",
    "# ========== STEP 4: USE THE ENSEMBLE ==========\n",
    "# Make predictions on new data\n",
    "predictions = stacked_ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "23f9158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STACKED ENSEMBLE WITH RIDGE META-LEARNER\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Base models: Ridge, lgbm\n",
      "  Number of models: 2\n",
      "  CV folds: 5\n",
      "  Optimize alpha: True\n",
      "  Alpha candidates: [0.1, 1.0, 10.0, 100.0]\n",
      "\n",
      "======================================================================\n",
      "STEP 1: GENERATING OUT-OF-FOLD PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "Processing Ridge...\n",
      "  ✅ Ridge complete                    \n",
      "\n",
      "Processing lgbm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 2/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 3/5 complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ lgbm complete                    \n",
      "\n",
      "✅ Out-of-fold predictions generated!\n",
      "   OOF shape: (3292, 2)\n",
      "   Val shape: (2689, 2)\n",
      "\n",
      "======================================================================\n",
      "STEP 2: TRAINING RIDGE META-LEARNER\n",
      "======================================================================\n",
      "\n",
      "Using RidgeCV to optimize alpha...\n",
      "  ✅ Best alpha found: 100.0000\n",
      "\n",
      "✅ Meta-learner trained!\n",
      "\n",
      "Meta-learner coefficients:\n",
      "  Ridge          :   1.2474\n",
      "  lgbm           :  -0.2772\n",
      "  Intercept      :  25.5397\n",
      "\n",
      "======================================================================\n",
      "STEP 3: MAKING PREDICTIONS\n",
      "======================================================================\n",
      "  ✅ Predictions generated\n",
      "\n",
      "======================================================================\n",
      "STEP 4: EVALUATING PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Training Performance:\n",
      "  RMSE: 1921.4253\n",
      "  MAE:  575.2878\n",
      "  R²:   0.6822\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 1331.5118\n",
      "  MAE:  473.1152\n",
      "  R²:   0.7429\n",
      "\n",
      "Overfitting Check:\n",
      "  RMSE gap: -589.9135 (-30.70%)\n",
      "  R² gap:   -0.0607\n",
      "  ✅ Low overfitting - model generalizes well\n",
      "\n",
      "======================================================================\n",
      "STEP 5: COMPARISON TO BASE MODELS\n",
      "======================================================================\n",
      "\n",
      "Individual model performance on validation set:\n",
      "  Ridge          : 1269.2618\n",
      "  lgbm           : 1431.0001\n",
      "\n",
      "Stacked ensemble:    1331.5118\n",
      "Best individual:     1269.2618\n",
      "Improvement:         -4.90%\n",
      "\n",
      "======================================================================\n",
      "STEP 6: CREATING ENSEMBLE OBJECT\n",
      "======================================================================\n",
      "\n",
      "⚠️  Warning: Small mismatch in predictions\n",
      "  Max difference: 3496.352140\n",
      "  This is likely due to floating point precision and is OK\n",
      "\n",
      "======================================================================\n",
      "✅ STACKED ENSEMBLE CREATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Final Performance:\n",
      "  Validation RMSE: 1331.5118\n",
      "  Validation R²: 0.7429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/clv_project/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_models = {\n",
    "    'Ridge': best_ridge,      # Your polynomial Ridge model\n",
    "    'lgbm': best_lgbm,          # Your LightGBM model\n",
    "}\n",
    "\n",
    "# ========== STEP 2: CREATE STACKED ENSEMBLE ==========\n",
    "result = create_stacked_ridge_ensemble(\n",
    "    base_models=base_models,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    selected_features=selected_features,\n",
    "    optimize_alpha=True,                    # Automatically find best alpha\n",
    "    alphas=[0.1, 1.0, 10.0, 100.0],        # Alpha values to try\n",
    "    n_folds=5,                              # Number of CV folds\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ========== STEP 3: ACCESS RESULTS ==========\n",
    "stacked_ensemble = result['ensemble']\n",
    "val_rmse = result['val_rmse']\n",
    "val_r2 = result['val_r2']\n",
    "coefficients = result['coefficients']\n",
    "\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"  Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"  Validation R²: {val_r2:.4f}\")\n",
    "\n",
    "# ========== STEP 4: USE THE ENSEMBLE ==========\n",
    "# Make predictions on new data\n",
    "predictions = stacked_ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1150cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clv_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
