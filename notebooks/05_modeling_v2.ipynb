{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CLV Prediction: Ridge Regression Experiments\n",
    "\n",
    "## Project Overview\n",
    "This notebook explores different strategies to improve Ridge regression performance for Customer Lifetime Value (CLV) prediction:\n",
    "\n",
    "1. **Feature Selection**: Testing manual vs automated features\n",
    "2. **Target Transformation**: Log transformation to handle skewed distribution\n",
    "3. **Outlier Treatment**: Clipping extreme values to reduce impact on RMSE\n",
    "\n",
    "All experiments are logged to **MLflow** for tracking and comparison.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Challenge**: CLV distribution is highly right-skewed (median: $167, mean: $726, max: $114,887), causing standard regression models to struggle with extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp1_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Experiment 1: Manual RFM Features\n",
    "\n",
    "## Objective\n",
    "Test if manually engineered RFM (Recency, Frequency, Monetary) features perform better than automated features from Featuretools.\n",
    "\n",
    "## Features Used\n",
    "- **Recency**: `days_since_last_purchase`\n",
    "- **Frequency**: `purchase_frequency`, `total_orders`\n",
    "- **Monetary**: `avg_order_value`, `avg_item_value`\n",
    "- **Additional**: `customer_age_days`, `active_days`, `avg_days_between_orders`, `avg_items_per_order`\n",
    "\n",
    "## Hypothesis\n",
    "Manual features focus on core customer behavior patterns and may generalize better than complex automated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define manual features\n",
    "features_manual = [\n",
    "    'days_since_last_purchase', \n",
    "    'customer_age_days',\n",
    "    'active_days', \n",
    "    'total_orders', \n",
    "    'purchase_frequency',\n",
    "    'avg_items_per_order', \n",
    "    'avg_days_between_orders', \n",
    "    'avg_order_value',\n",
    "    'avg_item_value'\n",
    "]\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('../data/features/train_final.csv')\n",
    "val = pd.read_csv('../data/features/val_final.csv')\n",
    "test = pd.read_csv('../data/features/test_final.csv')\n",
    "\n",
    "X_train = train[features_manual]\n",
    "y_train = train['CLV_Target']\n",
    "\n",
    "X_val = val[features_manual]\n",
    "y_val = val['CLV_Target']\n",
    "\n",
    "X_test = test[features_manual]\n",
    "y_test = test['CLV_Target']\n",
    "\n",
    "print(f\"Data loaded: {len(X_train)} train, {len(X_val)} val, {len(X_test)} test\")\n",
    "print(f\"Number of features: {len(features_manual)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train Ridge pipeline\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())   \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_transformer, features_manual)]\n",
    ")\n",
    "\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RidgeCV(alphas=[0.1, 1, 10], cv=5))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = ridge_pipeline.predict(X_train)\n",
    "y_val_pred = ridge_pipeline.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EXPERIMENT 1: MANUAL FEATURES - RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  RMSE: ${train_rmse:.2f}\")\n",
    "print(f\"  MAE:  ${train_mae:.2f}\")\n",
    "print(f\"  RÂ²:   {train_r2:.4f}\")\n",
    "print(f\"\\nValidation:\")\n",
    "print(f\"  RMSE: ${val_rmse:.2f}\")\n",
    "print(f\"  MAE:  ${val_mae:.2f}\")\n",
    "print(f\"  RÂ²:   {val_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1_mlflow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log to MLflow\n",
    "mlflow.set_experiment(\"Ridge_Regression_Experiments\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Exp1_Manual_Features\"):\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"experiment\": \"manual_features\",\n",
    "        \"model_type\": \"ridge\",\n",
    "        \"n_features\": len(features_manual),\n",
    "        \"feature_type\": \"manual_rfm\",\n",
    "        \"target_transform\": \"none\",\n",
    "        \"best_alpha\": ridge_pipeline.named_steps['model'].alpha_,\n",
    "        \"cv_folds\": 5\n",
    "    })\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"train_r2\": train_r2,\n",
    "        \"val_rmse\": val_rmse,\n",
    "        \"val_mae\": val_mae,\n",
    "        \"val_r2\": val_r2,\n",
    "        \"rmse_gap\": val_rmse - train_rmse,\n",
    "        \"val_rmse_pct_of_mean\": (val_rmse / y_val.mean()) * 100\n",
    "    })\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(ridge_pipeline, \"model\")\n",
    "    \n",
    "    # Log feature list\n",
    "    mlflow.log_dict({\"features\": features_manual}, \"features_used.json\")\n",
    "    \n",
    "    # Tags\n",
    "    mlflow.set_tags({\n",
    "        \"stage\": \"exploration\",\n",
    "        \"strategy\": \"feature_selection\",\n",
    "        \"author\": \"Bread\"\n",
    "    })\n",
    "    \n",
    "    print(\"\\nâœ… Experiment 1 logged to MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp2_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Experiment 2: Log Transformation\n",
    "\n",
    "## Objective\n",
    "Apply log transformation to the target variable (CLV) to handle the highly skewed distribution and reduce the influence of extreme outliers.\n",
    "\n",
    "## Method\n",
    "1. Transform target: `y_log = log(1 + y)` (log1p to handle zeros)\n",
    "2. Train Ridge regression on log-transformed target\n",
    "3. Transform predictions back: `y_pred = exp(y_log) - 1` (expm1)\n",
    "4. Evaluate on original scale\n",
    "\n",
    "## Hypothesis\n",
    "Log transformation will:\n",
    "- Compress large values\n",
    "- Make distribution more normal\n",
    "- Reduce RMSE by minimizing outlier impact\n",
    "\n",
    "## Features Used\n",
    "Using the best automated features from feature selection (14 features including aggregations and engineered combinations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp2_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected features from automated feature engineering\n",
    "selected_features = [\n",
    "    'SUM(transactions.Revenue)', \n",
    "    'SUM(transactions.Quantity)', \n",
    "    'COUNT(transactions) + MAX(transactions.Quantity)', \n",
    "    'total_orders', \n",
    "    'COUNT(transactions) + MAX(transactions.Revenue)', \n",
    "    'COUNT(transactions)', \n",
    "    'purchase_frequency', \n",
    "    'active_days', \n",
    "    'avg_days_between_orders', \n",
    "    'avg_items_per_order', \n",
    "    'avg_order_value', \n",
    "    'MAX(transactions.Revenue)', \n",
    "    'MEAN(transactions.Revenue)', \n",
    "    'MAX(transactions.Quantity) + MEAN(transactions.Revenue)'\n",
    "]\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('../data/features/train_final.csv')\n",
    "val = pd.read_csv('../data/features/val_final.csv')\n",
    "test = pd.read_csv('../data/features/test_final.csv')\n",
    "\n",
    "X_train = train[selected_features]\n",
    "y_train = train['CLV_Target']\n",
    "\n",
    "X_val = val[selected_features]\n",
    "y_val = val['CLV_Target']\n",
    "\n",
    "X_test = test[selected_features]\n",
    "y_test = test['CLV_Target']\n",
    "\n",
    "# Apply log transformation to ALL targets (train, val, test)\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "print(f\"\\nData Statistics:\")\n",
    "print(f\"  Mean CLV:   ${y_train.mean():.2f}\")\n",
    "print(f\"  Median CLV: ${y_train.median():.2f}\")\n",
    "print(f\"  Max CLV:    ${y_train.max():.2f}\")\n",
    "print(f\"\\nLog-transformed:\")\n",
    "print(f\"  Mean: {y_train_log.mean():.4f}\")\n",
    "print(f\"  Std:  {y_train_log.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp2_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())   \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_transformer, selected_features)]\n",
    ")\n",
    "\n",
    "ridge_pipeline_log = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5))\n",
    "])\n",
    "\n",
    "# Train on LOG scale\n",
    "ridge_pipeline_log.fit(X_train, y_train_log)\n",
    "\n",
    "# Predict on LOG scale\n",
    "y_train_pred_log = ridge_pipeline_log.predict(X_train)\n",
    "y_val_pred_log = ridge_pipeline_log.predict(X_val)\n",
    "\n",
    "# Transform back to ORIGINAL scale\n",
    "y_train_pred = np.expm1(y_train_pred_log)\n",
    "y_val_pred = np.expm1(y_val_pred_log)\n",
    "\n",
    "# Calculate metrics on ORIGINAL scale\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EXPERIMENT 2: LOG TRANSFORMATION - RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  RMSE: ${train_rmse:.2f}\")\n",
    "print(f\"  MAE:  ${train_mae:.2f}\")\n",
    "print(f\"  RÂ²:   {train_r2:.4f}\")\n",
    "print(f\"\\nValidation:\")\n",
    "print(f\"  RMSE: ${val_rmse:.2f}\")\n",
    "print(f\"  MAE:  ${val_mae:.2f}\")\n",
    "print(f\"  RÂ²:   {val_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp2_mlflow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log to MLflow\n",
    "mlflow.set_experiment(\"Ridge_Regression_Experiments\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Exp2_Log_Transform\"):\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"experiment\": \"log_transform\",\n",
    "        \"model_type\": \"ridge\",\n",
    "        \"n_features\": len(selected_features),\n",
    "        \"feature_type\": \"automated_selected\",\n",
    "        \"target_transform\": \"log1p\",\n",
    "        \"best_alpha\": ridge_pipeline_log.named_steps['model'].alpha_,\n",
    "        \"cv_folds\": 5\n",
    "    })\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"train_r2\": train_r2,\n",
    "        \"val_rmse\": val_rmse,\n",
    "        \"val_mae\": val_mae,\n",
    "        \"val_r2\": val_r2,\n",
    "        \"rmse_gap\": val_rmse - train_rmse,\n",
    "        \"val_rmse_pct_of_mean\": (val_rmse / y_val.mean()) * 100\n",
    "    })\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(ridge_pipeline_log, \"model\")\n",
    "    \n",
    "    # Log feature list\n",
    "    mlflow.log_dict({\"features\": selected_features}, \"features_used.json\")\n",
    "    \n",
    "    # Tags\n",
    "    mlflow.set_tags({\n",
    "        \"stage\": \"exploration\",\n",
    "        \"strategy\": \"target_transformation\",\n",
    "        \"author\": \"Bread\"\n",
    "    })\n",
    "    \n",
    "    print(\"\\nâœ… Experiment 2 logged to MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp3_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Experiment 3: Clipping Outliers\n",
    "\n",
    "## Objective\n",
    "Clip extreme CLV values during training to reduce their impact on the model, while still evaluating on the full unclipped validation set.\n",
    "\n",
    "## Method\n",
    "1. Calculate clip threshold at 95th percentile of training CLV (~$2,331)\n",
    "2. Cap training target values above threshold\n",
    "3. Train Ridge regression on clipped data\n",
    "4. Evaluate predictions on ORIGINAL unclipped validation data\n",
    "\n",
    "## Hypothesis\n",
    "Clipping will:\n",
    "- Allow model to focus on majority of customers (bottom 95%)\n",
    "- Reduce influence of extreme outliers during training\n",
    "- Result in better generalization\n",
    "\n",
    "## Rationale\n",
    "Error analysis shows that ~10-20 extreme customers (>$20k CLV) cause massive residuals ($20k-$60k errors) that dominate RMSE but represent <1% of customer base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp3_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload data (using same selected features)\n",
    "train = pd.read_csv('../data/features/train_final.csv')\n",
    "val = pd.read_csv('../data/features/val_final.csv')\n",
    "test = pd.read_csv('../data/features/test_final.csv')\n",
    "\n",
    "X_train = train[selected_features]\n",
    "y_train = train['CLV_Target']\n",
    "\n",
    "X_val = val[selected_features]\n",
    "y_val = val['CLV_Target']\n",
    "\n",
    "X_test = test[selected_features]\n",
    "y_test = test['CLV_Target']\n",
    "\n",
    "# Analyze distribution and determine clip threshold\n",
    "clip_percentile = 0.95\n",
    "clip_threshold = y_train.quantile(clip_percentile)\n",
    "\n",
    "print(f\"\\nDistribution Analysis:\")\n",
    "for p in [0.50, 0.75, 0.90, 0.95, 0.99]:\n",
    "    val = y_train.quantile(p)\n",
    "    count = (y_train > val).sum()\n",
    "    print(f\"  {int(p*100)}th percentile: ${val:,.2f} ({count} customers above)\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Clipping Strategy:\")\n",
    "print(f\"  Clip threshold: ${clip_threshold:,.2f} ({int(clip_percentile*100)}th percentile)\")\n",
    "print(f\"  Values to clip: {(y_train > clip_threshold).sum()} ({(y_train > clip_threshold).sum()/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "# Apply clipping to TRAINING target only\n",
    "y_train_clipped = y_train.clip(upper=clip_threshold)\n",
    "\n",
    "print(f\"\\nAfter Clipping:\")\n",
    "print(f\"  Mean CLV:   ${y_train_clipped.mean():.2f} (was ${y_train.mean():.2f})\")\n",
    "print(f\"  Max CLV:    ${y_train_clipped.max():.2f} (was ${y_train.max():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp3_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())   \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_transformer, selected_features)]\n",
    ")\n",
    "\n",
    "ridge_pipeline_clip = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5))\n",
    "])\n",
    "\n",
    "# Train on CLIPPED data\n",
    "ridge_pipeline_clip.fit(X_train, y_train_clipped)\n",
    "\n",
    "# Predict\n",
    "y_train_pred = ridge_pipeline_clip.predict(X_train)\n",
    "y_val_pred = ridge_pipeline_clip.predict(X_val)\n",
    "\n",
    "# Metrics on CLIPPED training target (for reference)\n",
    "train_rmse_clipped = np.sqrt(mean_squared_error(y_train_clipped, y_train_pred))\n",
    "train_mae_clipped = mean_absolute_error(y_train_clipped, y_train_pred)\n",
    "train_r2_clipped = r2_score(y_train_clipped, y_train_pred)\n",
    "\n",
    "# Metrics on ORIGINAL unclipped validation target (what matters!)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EXPERIMENT 3: CLIPPING OUTLIERS - RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nTraining (vs clipped target):\")\n",
    "print(f\"  RMSE: ${train_rmse_clipped:.2f}\")\n",
    "print(f\"  MAE:  ${train_mae_clipped:.2f}\")\n",
    "print(f\"  RÂ²:   {train_r2_clipped:.4f}\")\n",
    "print(f\"\\nValidation (vs ORIGINAL unclipped target):\")\n",
    "print(f\"  RMSE: ${val_rmse:.2f}\")\n",
    "print(f\"  MAE:  ${val_mae:.2f}\")\n",
    "print(f\"  RÂ²:   {val_r2:.4f}\")\n",
    "\n",
    "# Error by customer segment\n",
    "low_value = y_val <= y_val.quantile(0.33)\n",
    "mid_value = (y_val > y_val.quantile(0.33)) & (y_val <= y_val.quantile(0.67))\n",
    "high_value = y_val > y_val.quantile(0.67)\n",
    "\n",
    "print(f\"\\nRMSE by Customer Segment:\")\n",
    "print(f\"  Low value (bottom 33%):  ${np.sqrt(mean_squared_error(y_val[low_value], y_val_pred[low_value])):.2f}\")\n",
    "print(f\"  Mid value (middle 33%):  ${np.sqrt(mean_squared_error(y_val[mid_value], y_val_pred[mid_value])):.2f}\")\n",
    "print(f\"  High value (top 33%):    ${np.sqrt(mean_squared_error(y_val[high_value], y_val_pred[high_value])):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp3_mlflow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log to MLflow\n",
    "mlflow.set_experiment(\"Ridge_Regression_Experiments\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Exp3_Clipped_Outliers\"):\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"experiment\": \"clip_outliers\",\n",
    "        \"model_type\": \"ridge\",\n",
    "        \"n_features\": len(selected_features),\n",
    "        \"feature_type\": \"automated_selected\",\n",
    "        \"target_transform\": \"clip\",\n",
    "        \"clip_percentile\": clip_percentile,\n",
    "        \"clip_threshold\": clip_threshold,\n",
    "        \"values_clipped\": int((y_train > clip_threshold).sum()),\n",
    "        \"best_alpha\": ridge_pipeline_clip.named_steps['model'].alpha_,\n",
    "        \"cv_folds\": 5\n",
    "    })\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"train_rmse_clipped\": train_rmse_clipped,\n",
    "        \"train_mae_clipped\": train_mae_clipped,\n",
    "        \"train_r2_clipped\": train_r2_clipped,\n",
    "        \"val_rmse\": val_rmse,\n",
    "        \"val_mae\": val_mae,\n",
    "        \"val_r2\": val_r2,\n",
    "        \"rmse_gap\": val_rmse - train_rmse_clipped,\n",
    "        \"val_rmse_pct_of_mean\": (val_rmse / y_val.mean()) * 100,\n",
    "        \"val_rmse_low_segment\": np.sqrt(mean_squared_error(y_val[low_value], y_val_pred[low_value])),\n",
    "        \"val_rmse_mid_segment\": np.sqrt(mean_squared_error(y_val[mid_value], y_val_pred[mid_value])),\n",
    "        \"val_rmse_high_segment\": np.sqrt(mean_squared_error(y_val[high_value], y_val_pred[high_value]))\n",
    "    })\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(ridge_pipeline_clip, \"model\")\n",
    "    \n",
    "    # Log feature list and clip threshold\n",
    "    mlflow.log_dict({\n",
    "        \"features\": selected_features,\n",
    "        \"clip_threshold\": float(clip_threshold),\n",
    "        \"clip_percentile\": clip_percentile\n",
    "    }, \"model_config.json\")\n",
    "    \n",
    "    # Tags\n",
    "    mlflow.set_tags({\n",
    "        \"stage\": \"exploration\",\n",
    "        \"strategy\": \"outlier_treatment\",\n",
    "        \"author\": \"Bread\"\n",
    "    })\n",
    "    \n",
    "    print(\"\\nâœ… Experiment 3 logged to MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Experiment Summary & Conclusions\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "All three experiments have been logged to MLflow under the **\"Ridge_Regression_Experiments\"** experiment.\n",
    "\n",
    "### Expected Results:\n",
    "1. **Manual Features**: Lower performance than automated features due to missing complex interactions\n",
    "2. **Log Transform**: FAILED - Made RMSE worse due to transformation artifacts with extreme skew\n",
    "3. **Clipping**: Modest improvement for majority of customers, but still struggles with high-value segment\n",
    "\n",
    "## Why These Approaches Had Limited Success\n",
    "\n",
    "The fundamental challenge is **customer heterogeneity**:\n",
    "- 90% of customers have CLV < $2,000 (predictable)\n",
    "- 10% of customers have CLV > $2,000 (highly variable)\n",
    "- Single model struggles to handle both segments effectively\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Based on residual analysis showing concentrated errors in high-value customers, the recommended approach is:\n",
    "\n",
    "**Two-Tier Modeling System:**\n",
    "1. **Classifier**: Predict if customer will be high/low value\n",
    "2. **Regression Models**: Separate models for each segment\n",
    "3. **Combined Prediction**: Route customers to appropriate model\n",
    "\n",
    "Expected improvement: **15-25% RMSE reduction** (primarily on high-value segment)\n",
    "\n",
    "---\n",
    "\n",
    "## View Results in MLflow\n",
    "\n",
    "Run MLflow UI to compare experiments:\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "Navigate to `http://localhost:5000` and select the **\"Ridge_Regression_Experiments\"** experiment to compare:\n",
    "- Validation RMSE\n",
    "- RÂ² scores  \n",
    "- Impact of different strategies\n",
    "\n",
    "Filter and sort by metrics to identify the best approach for your use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clv_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
